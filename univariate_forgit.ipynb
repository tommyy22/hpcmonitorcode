{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# conn =sqlite3.connect('sqlite_8.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "# print(c.fetchall())\n",
    "\n",
    "# c.execute(\"SELECT * FROM syslogs ORDER BY timestamp DESC LIMIT 1\")\n",
    "# print(c.fetchall())\n",
    "\n",
    "\n",
    "limit = '16'\n",
    "bucket_size = 10\n",
    "timing =90\n",
    "\n",
    "\n",
    "variable = int(60*bucket_size)\n",
    "\n",
    "df= pd.read_pickle(\"dataframe.gz\")\n",
    "\n",
    "\n",
    "def makeDataframe(df):\n",
    "    import datetime\n",
    "\n",
    "    atype = str(df.tail(1)['bucket'].values[0])\n",
    "    ctype = str(df.head(1)['bucket'].values[0])\n",
    "\n",
    "    sample_series = []\n",
    "\n",
    "    end = datetime.datetime.strptime(atype, '%Y-%m-%d %H:%M:%S')\n",
    "    # start = end - datetime.timedelta(minutes = 3000)\n",
    "    start = datetime.datetime.strptime(ctype, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    delta = datetime.timedelta(minutes=bucket_size)\n",
    "    # start = datetime.datetime.strptime( start, '%Y-%m-%d %H:%M:%S' )\n",
    "    # end = datetime.datetime.strptime( end, '%Y-%m-%d %H:%M:%S' )\n",
    "    t = start\n",
    "    while t <= end :\n",
    "        sample_series.append(datetime.datetime.strftime( t, '%Y-%m-%d %H:%M:%S'))\n",
    "        t += delta\n",
    "    sample = pd.DataFrame(sample_series)\n",
    "    sample =sample.rename(columns={0:\"bucket\"})\n",
    "\n",
    "    sample.insert(1, 'msg_freq',0)\n",
    "\n",
    "    merged_df = df.merge(sample, how = 'outer', on = ['bucket'])\n",
    "    new_df = merged_df.sort_values('bucket')\n",
    "    new_df = new_df.drop(columns = ['msg_freq_y']).fillna(0)\n",
    "\n",
    "    new_df  = new_df.reset_index()\n",
    "    new_df = new_df.iloc[:,1:]\n",
    "    df = new_df.rename(columns={\"msg_freq_x\": \"msg_freq\"})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cumulativeFreq(df):\n",
    "    df_cs = df.copy()\n",
    "    df_cs['bucket'] = pd.to_datetime(df_cs['bucket'])\n",
    "    df_cs[\"msg_freq\"] = df_cs[\"msg_freq\"].iloc[:].groupby([df_cs['bucket'].dt.hour, df_cs['bucket'].dt.date]).cumsum()\n",
    "    \n",
    "    return df_cs\n",
    "\n",
    "\n",
    "\n",
    "def Normalizer(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    df1 = df.iloc[:,1:]   #change required\n",
    "#     values = dff.values\n",
    "#     values = values.astype('float32')\n",
    "#     # normalize features\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaled = scaler.fit_transform(values)\n",
    "#     dff = pd.DataFrame(scaled, columns=df.columns[1:]) #change required\n",
    "    dff = df1/df1.max()\n",
    "    dff.insert(0, 'bucket', df[\"bucket\"]) #change required\n",
    "    \n",
    "    return dff\n",
    "\n",
    "\n",
    "def sigmoidDataframe(df):\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        a=0.7\n",
    "        return 1/(1+5*np.exp(-a*x))\n",
    "    \n",
    "    dff = df.copy()\n",
    "    dff['msg_freq'] = sigmoid(dff['msg_freq'])\n",
    "    return dff\n",
    "\n",
    "df = makeDataframe(df)\n",
    "df_cs = cumulativeFreq(df)\n",
    "\n",
    "dfn = Normalizer(df)\n",
    "dfn_cs = Normalizer(df_cs)\n",
    "\n",
    "\n",
    "\n",
    "#Creating a synthesized dataset\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "sample_series = []\n",
    "x = np.array([[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55]])\n",
    "z =x\n",
    "# z=np.array([])\n",
    "for i in range(110):\n",
    "    y =np.random.rand(x.shape[1])*30\n",
    "#     z=np.append(np.append(z,y),x)\n",
    "#     z=np.append(z,x)\n",
    "    z=np.append(z,x+y)\n",
    "\n",
    "dfs = pd.DataFrame(z)\n",
    "\n",
    "zeit ='2022-01-10 18:00:00'\n",
    "\n",
    "end = datetime.datetime.strptime(zeit, '%Y-%m-%d %H:%M:%S')\n",
    "start = end - datetime.timedelta(minutes = int((len(dfs)-1)*10))\n",
    "# start = datetime.datetime.strptime(ctype, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "delta = datetime.timedelta(minutes=10)\n",
    "\n",
    "t = start\n",
    "while t <= end :\n",
    "    sample_series.append(datetime.datetime.strftime( t, '%Y-%m-%d %H:%M:%S'))\n",
    "    t += delta\n",
    "syn_df = pd.DataFrame(sample_series)\n",
    "syn_df =syn_df.rename(columns={0:\"bucket\"})\n",
    "\n",
    "syn_df['values']=dfs\n",
    "\n",
    "syn_dfn = Normalizer(syn_df)\n",
    "\n",
    "\n",
    "Values = df_cs.iloc[:,1].values\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "# choose a number of time steps; basically the memory of the network\n",
    "n_steps = int(timing/bucket_size)\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(Values, n_steps)\n",
    "\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "\n",
    "def wisesplit(num,div):\n",
    "    \n",
    "    part = num - div\n",
    "    while part/num > 0.8:\n",
    "        part = part-div\n",
    "    \n",
    "    return part\n",
    "\n",
    "def print_factors(num):\n",
    "    import numpy as np\n",
    "    factors = []\n",
    "    for i in range(1, num + 1):\n",
    "        if num % i == 0:\n",
    "            factors.append(i)\n",
    "    return np.array(factors)\n",
    "\n",
    "arr =print_factors(len(X))\n",
    "div = arr[int(0.6*arr.shape[0])]\n",
    "\n",
    "split = wisesplit(len(X),div)\n",
    "# split =int(0.8*len(X))\n",
    "# split = -272\n",
    "\n",
    "train_X = np.array(X)[:split]\n",
    "train_y = np.array(y)[:split]\n",
    "test_X = np.array(X)[split:]\n",
    "test_y = np.array(y)[split:]\n",
    "\n",
    "train_y =train_y.reshape((train_y.shape[0],1))\n",
    "\n",
    "test_y = test_y.reshape((test_y.shape[0],1))\n",
    "\n",
    "def compute_hcf(x, y):\n",
    "\n",
    "# choose the smaller number\n",
    "    if x > y:\n",
    "        smaller = y\n",
    "    else:\n",
    "        smaller = x\n",
    "    for i in range(1, smaller+1):\n",
    "        if((x % i == 0) and (y % i == 0)):\n",
    "            hcf = i \n",
    "    return hcf\n",
    "\n",
    "batchsize = compute_hcf(train_X.shape[0],test_X.shape[0])\n",
    "batch_arr = print_factors(batchsize)\n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "\n",
    "def Model(lr,epoch, batchsize):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, batch_input_shape=( batchsize,n_steps, n_features),return_sequences=False, stateful=True))\n",
    "#     model.add(LSTM(96,return_sequences=True))\n",
    "#     model.add(LSTM(64))\n",
    "    model.add(Dense(1))\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_logarithmic_error')\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    \n",
    "    history = model.fit(x=train_X, y=train_y, epochs=epoch, validation_data=(test_X,test_y), batch_size=batchsize, verbose =1, shuffle=False)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "lr_array = np.array([0.0001])\n",
    "epoch_array = np.array([200])\n",
    "batch_array = batch_arr[-1:]\n",
    "print(batch_array)\n",
    "\n",
    "for lr in lr_array:\n",
    "    for ep in epoch_array:\n",
    "        for bt in batch_array:\n",
    "            \n",
    "            print(\"\\n\\nNumber of time steps: {}\".format(n_steps))\n",
    "            print(\"Time bucket size: {}\".format(bucket_size))\n",
    "            print(\"Number of Epochs = {}\".format(ep))\n",
    "            print(\"Learning Rate = {}\".format(lr))\n",
    "            print(\"Batch size = {}\\n\\n\".format(bt))\n",
    "            \n",
    "            model,history = Model(lr,ep,bt)\n",
    "\n",
    "            y_train_pred = model.predict(train_X, batch_size=bt)\n",
    "            y_train_pred = y_train_pred.reshape(y_train_pred.shape[0],1)\n",
    "            train_mae_loss = np.abs(y_train_pred - train_y)\n",
    "\n",
    "            threshold = np.amax(train_mae_loss)\n",
    "            print(threshold)\n",
    "\n",
    "\n",
    "            y_test_pred = model.predict(test_X, batch_size=bt)\n",
    "            y_test_pred = y_test_pred.reshape(y_test_pred.shape[0],1)\n",
    "            test_mae_loss = np.abs(y_test_pred - test_y)\n",
    "\n",
    "\n",
    "            test_score_df = pd.DataFrame(test_mae_loss)\n",
    "            test_score_df['threshold'] = threshold\n",
    "            test_score_df['anomaly'] = test_score_df[0] > test_score_df['threshold']\n",
    "            anomalies = test_score_df.loc[test_score_df['anomaly'] == True]\n",
    "            print(anomalies.shape)\n",
    "\n",
    "            test_anom = pd.DataFrame(test_y)\n",
    "            pred_df = pd.DataFrame(y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "            #PLOTTING\n",
    "\n",
    "            plt.figure(figsize=(12,8))   \n",
    "            plt.plot(history.history['loss'],label='Training loss')\n",
    "            plt.plot(history.history['val_loss'], label =\"Validation loss\")\n",
    "            plt.xlabel('Epochs', fontsize=10)\n",
    "            plt.ylabel('Loss', fontsize=10)\n",
    "#             plt.ylim(0,0.2)\n",
    "            plt.legend(['Training loss', 'Validation loss'])\n",
    "            plt.savefig('1e-4_10_90_layer1.pdf', dpi=250)\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "\n",
    "#             plt.figure()    \n",
    "#             plt.plot(history.history['acc'],label='Training  accuracy')\n",
    "#             plt.plot(history.history['val_acc'], label =\"Validation accuracy\")\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "            fig = go.Figure()    \n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= test_anom[0], name='actual'))\n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= pred_df[0], name= 'prediction',line=dict(dash ='dot')))\n",
    "            fig.update_layout(showlegend=True, title='Frequency v/s Time',width=1600, height=800)\n",
    "            fig.update_yaxes(title_text='Normalized frequency')\n",
    "            fig.update_xaxes(title_text='Time')\n",
    "            fig.update_layout(xaxis_range=['2022-01-09 12:00:00','2022-01-10 00:00:00'])\n",
    "#             fig.update_layout(yaxis_range=[-0.1,1.1])\n",
    "#             fig.update_xaxes(title_text='Timestamp')\n",
    "            fig.show()\n",
    "            fig.write_image(\"p_1e-4_10_90_layer1.pdf\")\n",
    "\n",
    "#             fig = go.Figure()    \n",
    "#             fig.add_trace(go.Scatter(x=syn_df['bucket'][split+n_steps:], y= np.abs(test_anom[0]-pred_df[0])/test_anom[0], name='error'))\n",
    "#             fig.update_layout(showlegend=True, title='Absolute Error for test data')\n",
    "#             fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
