{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "\n",
    "# conn =sqlite3.connect('sqlite_9.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "# print(c.fetchall())\n",
    "\n",
    "# c.execute(\"SELECT * FROM syslogs ORDER BY timestamp DESC LIMIT 1\")\n",
    "# datei = ' '.join(c.fetchall()[0][2:4])\n",
    "\n",
    "y1,y2,y3,y4=[],[],[],[]\n",
    "z1,z2,z3,z4=[],[],[],[]\n",
    "limit = '16'\n",
    "bucket_size = 10\n",
    "timing =90\n",
    "\n",
    "\n",
    "variable = int(60*bucket_size)\n",
    "\n",
    "df_1 = pd.read_pickle(\"dataframe1_multi.gz\")\n",
    "df_2 = pd.read_pickle(\"dataframe2_multi.gz\")\n",
    "\n",
    "def MergeDataframe(df_1,df_2):\n",
    "    \n",
    "    #Finding the start and end time form the two dataframes\n",
    "    from datetime import datetime\n",
    "\n",
    "    atype = str(df_1.tail(1)['bucket'].values[0])\n",
    "    btype = str(df_2.tail(1)['bucket'].values[0])\n",
    "    if datetime.strptime(atype, '%Y-%m-%d %H:%M:%S')<=datetime.strptime(btype, '%Y-%m-%d %H:%M:%S'):\n",
    "        atype = btype\n",
    "\n",
    "    ctype = str(df_1.head(1)['bucket'].values[0])\n",
    "    dtype = str(df_2.head(1)['bucket'].values[0])\n",
    "    if datetime.strptime(ctype, '%Y-%m-%d %H:%M:%S')>=datetime.strptime(dtype, '%Y-%m-%d %H:%M:%S'):\n",
    "        ctype = dtype\n",
    "\n",
    "\n",
    "\n",
    "    #Merge emply dataframe with queried data for df_1\n",
    "\n",
    "    import datetime\n",
    "    sample_series = []\n",
    "\n",
    "    end = datetime.datetime.strptime(atype, '%Y-%m-%d %H:%M:%S')\n",
    "    # start = end - datetime.timedelta(minutes = 3000)\n",
    "    start = datetime.datetime.strptime(ctype, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    delta = datetime.timedelta(minutes=bucket_size)\n",
    "    # start = datetime.datetime.strptime( start, '%Y-%m-%d %H:%M:%S' )\n",
    "    # end = datetime.datetime.strptime( end, '%Y-%m-%d %H:%M:%S' )\n",
    "    t = start\n",
    "    while t <= end :\n",
    "        sample_series.append(datetime.datetime.strftime( t, '%Y-%m-%d %H:%M:%S'))\n",
    "        t += delta\n",
    "    sample = pd.DataFrame(sample_series)\n",
    "    sample =sample.rename(columns={0:\"bucket\"})\n",
    "\n",
    "    sample.insert(1, 'avg_s',0)\n",
    "    sample.insert(2, 'avg_f',0)\n",
    "    sample.insert(3, 'msg_freq_top10',0)\n",
    "    sample.insert(4, 'msg_freq_nontop10',0)\n",
    "\n",
    "\n",
    "    merged_df = df_1.merge(sample, how = 'outer', on = ['bucket'])\n",
    "    new_df = merged_df.sort_values('bucket')\n",
    "    new_df = new_df.drop(columns = ['avg_s_y', 'avg_f_y', 'msg_freq_top10', 'msg_freq_nontop10']).fillna(0)\n",
    "\n",
    "    new_df  = new_df.reset_index()\n",
    "    new_df = new_df.iloc[:,1:]\n",
    "    df_1 = new_df.rename(columns={\"avg_s_x\": \"avg_s\", \"avg_f_x\": \"avg_f\"})\n",
    "    df_1 = df_1[[\"bucket\", \"avg_s\", \"avg_f\", \"msg_freq\"]]\n",
    "\n",
    "    merged_df = df_2.merge(sample, how = 'outer', on = ['bucket'])\n",
    "    new_df = merged_df.sort_values('bucket')\n",
    "    new_df = new_df.drop(columns = ['avg_s_y', 'avg_f_y', 'msg_freq_top10', 'msg_freq_nontop10']).fillna(0)\n",
    "\n",
    "    new_df  = new_df.reset_index()\n",
    "    new_df = new_df.iloc[:,1:]\n",
    "    df_2 = new_df.rename(columns={\"avg_s_x\": \"avg_s\", \"avg_f_x\": \"avg_f\"})\n",
    "    df_2 = df_2[[\"bucket\", \"avg_s\", \"avg_f\", \"msg_freq\"]]\n",
    "\n",
    "    #DATAFRAME WITH NON-NORMALIZED VALUES (df_final)\n",
    "\n",
    "    df_final = df_1 \n",
    "    df_final = df_final.drop(columns=[\"msg_freq\"])\n",
    "    df_final[\"msg_freq_top10\"] = df_1[\"msg_freq\"]\n",
    "    df_final[\"msg_freq_nontop10\"] = df_2[\"msg_freq\"]\n",
    "    df_final[\"avg_s\"] = (df_1[\"avg_s\"]*df_1[\"msg_freq\"]+df_2[\"avg_s\"]*df_2[\"msg_freq\"])/(df_1[\"msg_freq\"]+df_2[\"msg_freq\"])\n",
    "    df_final[\"avg_f\"] = (df_1[\"avg_f\"]*df_1[\"msg_freq\"]+df_2[\"avg_f\"]*df_2[\"msg_freq\"])/(df_1[\"msg_freq\"]+df_2[\"msg_freq\"])\n",
    "\n",
    "    df_final  = df_final.fillna(0)\n",
    "    return df_final\n",
    "\n",
    "def Cummulative(df):\n",
    "    df_cs = df.copy()\n",
    "    df_cs['bucket'] = pd.to_datetime(df_cs['bucket'])\n",
    "    df_cs[\"msg_freq_top10\"] = df_cs[\"msg_freq_top10\"].iloc[:].groupby([df_cs['bucket'].dt.hour, df_cs['bucket'].dt.date]).cumsum()\n",
    "    df_cs[\"msg_freq_nontop10\"] = df_cs[\"msg_freq_nontop10\"].iloc[:].groupby([df_cs['bucket'].dt.hour, df_cs['bucket'].dt.date]).cumsum()\n",
    "    \n",
    "    return df_cs\n",
    "\n",
    "#NORMALIZED DATAFRAME WITH MIN-MAX SCALER (dff)\n",
    "\n",
    "def Normalize(df):\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    df1 = df.iloc[:,1:5]   #change required\n",
    "    values = df1.values\n",
    "    values = values.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0.5, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    dff = pd.DataFrame(scaled, columns=df.columns[1:]) #change required\n",
    "#     dff = df1/df1.max()\n",
    "    dff.insert(0, 'bucket', df[\"bucket\"]) #change required\n",
    "\n",
    "    return dff\n",
    "\n",
    "def SigmoidNormal(df):\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        a=0.7\n",
    "        return 1/(1+5*np.exp(-a*x))\n",
    "    def normalize(x):\n",
    "        return x/x.max()\n",
    "\n",
    "    dff_1 = df.copy()  #change required\n",
    "    dff_1[['msg_freq_top10', 'msg_freq_nontop10']] = sigmoid(dff_1[['msg_freq_top10', 'msg_freq_nontop10']])\n",
    "    dff_1[['avg_s', 'avg_f']] = normalize(dff_1[['avg_s', 'avg_f']])\n",
    "    \n",
    "    return dff_1\n",
    "\n",
    "df = MergeDataframe(df_1,df_2)\n",
    "\n",
    "#reversing priority\n",
    "df['avg_s'] = 7-df['avg_s']\n",
    "df['avg_f'] = 8-df['avg_f']\n",
    "\n",
    "df_cs = Cummulative(df)\n",
    "\n",
    "dfn = Normalize(df)\n",
    "dfn_cs = Normalize(df_cs)\n",
    "\n",
    "#Creating a synthesized dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "sample_series = []\n",
    "x = np.array([[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55]])\n",
    "z =x\n",
    "# z=np.array([])\n",
    "for i in range(60):\n",
    "    y =np.random.rand(x.shape[1])*30\n",
    "#     z=np.append(np.append(z,y),x)\n",
    "#     z=np.append(z,y)\n",
    "    z=np.append(z,x)\n",
    "\n",
    "dfs = pd.DataFrame(z)\n",
    "\n",
    "zeit ='2022-01-29 18:00:00'\n",
    "\n",
    "end = datetime.datetime.strptime(zeit, '%Y-%m-%d %H:%M:%S')\n",
    "start = end - datetime.timedelta(minutes = int((len(dfs)-1)*10))\n",
    "# start = datetime.datetime.strptime(ctype, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "delta = datetime.timedelta(minutes=10)\n",
    "\n",
    "t = start\n",
    "while t <= end :\n",
    "    sample_series.append(datetime.datetime.strftime( t, '%Y-%m-%d %H:%M:%S'))\n",
    "    t += delta\n",
    "syn_df = pd.DataFrame(sample_series)\n",
    "syn_df =syn_df.rename(columns={0:\"bucket\"})\n",
    "\n",
    "syn_df['values0']=dfs\n",
    "syn_df['values1']=1/(dfs+1)\n",
    "syn_df['values2']=dfs*2\n",
    "syn_df['values3']=dfs*dfs\n",
    "\n",
    "syn_dfn = Normalize(syn_df)\n",
    "\n",
    "#CHOOSING WHICH DATAFRAME TO TAKE AS INPUT\n",
    "Values = dfn_cs.iloc[:,1:5].values\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix,:], sequence[end_ix,:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# choose a number of time steps; basically the memory of the network\n",
    "n_steps = int(timing/bucket_size)\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(Values, n_steps)\n",
    "\n",
    "n_features = 4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wisesplit(num,div):\n",
    "    \n",
    "    part = num - div\n",
    "    while part/num > 0.95:\n",
    "        part = part-div\n",
    "    \n",
    "    return part\n",
    "\n",
    "def print_factors(num):\n",
    "    import numpy as np\n",
    "    factors = []\n",
    "    for i in range(1, num + 1):\n",
    "        if num % i == 0:\n",
    "            factors.append(i)\n",
    "    return np.array(factors)\n",
    "\n",
    "arr =print_factors(len(X))\n",
    "if arr.shape[0]==2:\n",
    "    div = arr[0]\n",
    "else:\n",
    "    div = arr[int(0.6*arr.shape[0])]\n",
    "\n",
    "split = wisesplit(len(X),div)\n",
    "# split =int(0.8*len(X))\n",
    "# split = -256\n",
    "\n",
    "train_X = np.array(X)[:split]\n",
    "train_y = np.array(y)[:split]\n",
    "test_X = np.array(X)[split:]\n",
    "test_y = np.array(y)[split:]\n",
    "\n",
    "train_y =train_y.reshape((train_y.shape[0],1,train_y.shape[1]))\n",
    "# test_X = test_X.reshape((1, test_X.shape[0], test_X.shape[1]))\n",
    "test_y = test_y.reshape((test_y.shape[0],1,test_y.shape[1]))\n",
    "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "\n",
    "def compute_hcf(x, y):\n",
    "\n",
    "# choose the smaller number\n",
    "    if x > y:\n",
    "        smaller = y\n",
    "    else:\n",
    "        smaller = x\n",
    "    for i in range(1, smaller+1):\n",
    "        if((x % i == 0) and (y % i == 0)):\n",
    "            hcf = i \n",
    "    return hcf\n",
    "\n",
    "batchsize = compute_hcf(train_X.shape[0],test_X.shape[0])\n",
    "batch_arr = print_factors(batchsize)\n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "# test_X\n",
    "\n",
    "def Model(lr,epoch, batchsize):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, batch_input_shape=( batchsize,n_steps, n_features),stateful=True))\n",
    "    model.add(Dense(4))\n",
    "    opt = keras.optimizers.Adam(learning_rate= lr)\n",
    "    #     model.add(LSTM(100,activation= 'relu'))\n",
    "#     model.add(Dropout(rate=0.1))  \n",
    "    #     model.add(RepeatVector(n=1))\n",
    "    model.compile(loss='msle', optimizer=opt)\n",
    "#     print(model.summary())\n",
    "\n",
    "    history = model.fit(x=train_X, y=train_y, epochs=epoch, validation_data=(test_X,test_y), batch_size=batchsize,verbose =1, shuffle=False)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "lr_array = np.array([0.001])\n",
    "epoch_array = np.array([500])\n",
    "batch_array = batch_arr[-1:]\n",
    "print(batch_array)\n",
    "\n",
    "for lr in lr_array:\n",
    "    for ep in epoch_array:\n",
    "        for bt in batch_array:\n",
    "            \n",
    "            print(\"\\n\\nNumber of Epochs = {}\".format(ep))\n",
    "            print(\"Learning Rate = {}\".format(lr))\n",
    "            print(\"Batch size = {}\\n\\n\".format(bt))\n",
    "            \n",
    "            model,history = Model(lr,ep,bt)\n",
    "            \n",
    "            y_train_pred = model.predict(train_X, batch_size = bt)\n",
    "            y_train_pred = y_train_pred.reshape(y_train_pred.shape[0], 1,y_train_pred.shape[1])\n",
    "            train_mae_loss = np.abs(y_train_pred - train_y)\n",
    "\n",
    "            threshold_avgs = np.amax(train_mae_loss[:,0,0])\n",
    "            threshold_avgf = np.amax(train_mae_loss[:,0,1])\n",
    "            threshold_freqt10 = np.amax(train_mae_loss[:,0,2])\n",
    "            threshold_freqnt10 = np.amax(train_mae_loss[:,0,3])\n",
    "\n",
    "            print(threshold_avgs,threshold_avgf, threshold_freqt10,threshold_freqnt10 )\n",
    "\n",
    "            y_test_pred = model.predict(test_X, batch_size = bt)\n",
    "            y_test_pred = y_test_pred.reshape(y_test_pred.shape[0], 1,y_test_pred.shape[1])\n",
    "            # test_y = test_y.reshape(test_y.shape[0],1,test_y.shape[1])\n",
    "            test_mae_loss = np.abs(y_test_pred - test_y)\n",
    "\n",
    "\n",
    "            test_score_df = pd.DataFrame(test_mae_loss[:,0,:])\n",
    "\n",
    "            test_score_df['threshold_avgs'] = threshold_avgs\n",
    "            test_score_df['threshold_avgf'] = threshold_avgf\n",
    "            test_score_df['threshold_freqt10'] = threshold_freqt10\n",
    "            test_score_df['threshold_freqnt10'] = threshold_freqnt10\n",
    "            test_score_df['anomaly_avgs'] = test_score_df[0] > test_score_df['threshold_avgs']\n",
    "            test_score_df['anomaly_avgf'] = test_score_df[1] > test_score_df['threshold_avgf']\n",
    "            test_score_df['anomaly_freqt10'] = test_score_df[2] > test_score_df['threshold_freqt10']\n",
    "            test_score_df['anomaly_freqnt10'] = test_score_df[3] > test_score_df['threshold_freqnt10']\n",
    "\n",
    "\n",
    "            anomalies_avgs = test_score_df.loc[test_score_df['anomaly_avgs'] == True]\n",
    "            print(anomalies_avgs.shape)\n",
    "\n",
    "            anomalies_avgf = test_score_df.loc[test_score_df['anomaly_avgf'] == True]\n",
    "            print(anomalies_avgf.shape)\n",
    "\n",
    "            anomalies_freqt10 = test_score_df.loc[test_score_df['anomaly_freqt10'] == True]\n",
    "            print(anomalies_freqt10.shape)\n",
    "\n",
    "            anomalies_freqnt10 = test_score_df.loc[test_score_df['anomaly_freqnt10'] == True]\n",
    "            print(anomalies_freqnt10.shape)\n",
    "\n",
    "            test_anom = pd.DataFrame(test_y[:,0,:])\n",
    "            pred_df = pd.DataFrame(y_test_pred[:,0,:])\n",
    "\n",
    "            test_anom['avgs']=anomalies_avgs[0]\n",
    "            test_anom['avgf']=anomalies_avgf[1]\n",
    "            test_anom['freqt10'] = anomalies_freqt10[2]\n",
    "            test_anom['freqnt10'] = anomalies_freqnt10[3]\n",
    "\n",
    "            y1.append(test_anom[0])\n",
    "            y2.append(test_anom[1])\n",
    "            y3.append(test_anom[2])\n",
    "            y4.append(test_anom[3])\n",
    "\n",
    "            z1.append(pred_df[0])\n",
    "            z2.append(pred_df[1])\n",
    "            z3.append(pred_df[2])\n",
    "            z4.append(pred_df[3])\n",
    "\n",
    "            #PLOTTING\n",
    "\n",
    "            plt.figure(figsize=(12,8))   \n",
    "            plt.plot(history.history['loss'],label='Training loss')\n",
    "            plt.plot(history.history['val_loss'], label =\"Validation loss\")\n",
    "            plt.xlabel('Epochs', fontsize=10)\n",
    "            plt.ylabel('Loss', fontsize=10)\n",
    "#             plt.ylim(0,0.5)\n",
    "            plt.legend(['Training loss', 'Validation loss'])\n",
    "#             plt.savefig('C:/Users/tomri/Desktop/vector save/plot16/plot16_32.pdf', dpi=250)\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "\n",
    "            fig = go.Figure()    \n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= test_anom[0], name='Actual'))\n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= pred_df[0], name ='Prediction',line=dict(dash ='dot')))\n",
    "            fig.update_layout(showlegend=True, title='Average severity v/s Time ')\n",
    "            fig.update_yaxes(title_text='Normalized average severity')\n",
    "            fig.update_xaxes(title_text='Time')\n",
    "            fig.show()\n",
    "            \n",
    "            \n",
    "            fig = go.Figure()    \n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= test_anom[1], name='Actual'))\n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= pred_df[1], name ='Prediction',line=dict(dash ='dot')))\n",
    "            fig.update_layout(showlegend=True, title='Average facility v/s Time')\n",
    "            fig.update_yaxes(title_text='Normalized average facility')\n",
    "            fig.update_xaxes(title_text='Time')\n",
    "            fig.show()\n",
    "\n",
    "            \n",
    "            fig = go.Figure()    \n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= test_anom[2], name='Actual'))\n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= pred_df[2], name ='Prediction',line=dict(dash ='dot')))\n",
    "            fig.update_layout(showlegend=True, title='Top 10 message count v/s Time')\n",
    "            fig.update_yaxes(title_text='Normalized frequency of top 10 messages')\n",
    "            fig.update_xaxes(title_text='Time')\n",
    "            fig.show()\n",
    "\n",
    "            \n",
    "            fig = go.Figure()    \n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= test_anom[3], name='Actual'))\n",
    "            fig.add_trace(go.Scatter(x=dfn_cs['bucket'][split+n_steps:], y= pred_df[3], name ='Prediction',line=dict(dash ='dot')))\n",
    "            fig.update_layout(showlegend=True, title='Non-top 10 messages v/s Time')\n",
    "            fig.update_yaxes(title_text='Normalized frequency of non-top 10 messages')\n",
    "            fig.update_xaxes(title_text='Time')\n",
    "            fig.show()\n",
    "            \n",
    "            #Synthezised Dataframe\n",
    "\n",
    "            \n",
    "#             fig = go.Figure()    \n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= test_anom[0], name='Actual'))\n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= pred_df[0], name ='Prediction'))\n",
    "#             fig.update_layout(showlegend=True, title='Average severity v/s Time ')\n",
    "#             fig.update_yaxes(title_text='Normalized average severity')\n",
    "#             fig.show()\n",
    "            \n",
    "            \n",
    "#             fig = go.Figure()    \n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= test_anom[1], name='Actual'))\n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= pred_df[1], name ='Prediction'))\n",
    "#             fig.update_layout(showlegend=True, title='Average facility v/s Time')\n",
    "#             fig.update_yaxes(title_text='Normalized average facility')\n",
    "#             fig.show()\n",
    "\n",
    "            \n",
    "#             fig = go.Figure()    \n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= test_anom[2], name='Actual'))\n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= pred_df[2], name ='Prediction'))\n",
    "#             fig.update_layout(showlegend=True, title='Top 10 message count v/s Time')\n",
    "#             fig.update_yaxes(title_text='Normalized frequency')\n",
    "#             fig.show()\n",
    "\n",
    "            \n",
    "#             fig = go.Figure()    \n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= test_anom[3], name='Actual'))\n",
    "#             fig.add_trace(go.Scatter(x=syn_dfn['bucket'][split+n_steps:], y= pred_df[3], name ='Prediction'))\n",
    "#             fig.update_layout(showlegend=True, title='Non-top 10 messages v/s Time')\n",
    "#             fig.update_yaxes(title_text='Normalized frequency')\n",
    "#             fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
