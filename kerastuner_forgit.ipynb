{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "# conn =sqlite3.connect('sqlite_8.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "# print(c.fetchall())\n",
    "\n",
    "# c.execute(\"SELECT * FROM syslogs ORDER BY timestamp DESC LIMIT 1\")\n",
    "# print(c.fetchall())\n",
    "\n",
    "\n",
    "limit = '16'\n",
    "bucket_size = 10\n",
    "timing =90\n",
    "\n",
    "\n",
    "variable = int(60*bucket_size)\n",
    "\n",
    "df = pd.read_pickle(\"dataframe.gz\")\n",
    "\n",
    "\n",
    "def makeDataframe(df):\n",
    "    import datetime\n",
    "\n",
    "    atype = str(df.tail(1)['bucket'].values[0])\n",
    "    ctype = str(df.head(1)['bucket'].values[0])\n",
    "\n",
    "    sample_series = []\n",
    "\n",
    "    end = datetime.datetime.strptime(atype, '%Y-%m-%d %H:%M:%S')\n",
    "    # start = end - datetime.timedelta(minutes = 3000)\n",
    "    start = datetime.datetime.strptime(ctype, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    delta = datetime.timedelta(minutes=bucket_size)\n",
    "    # start = datetime.datetime.strptime( start, '%Y-%m-%d %H:%M:%S' )\n",
    "    # end = datetime.datetime.strptime( end, '%Y-%m-%d %H:%M:%S' )\n",
    "    t = start\n",
    "    while t <= end :\n",
    "        sample_series.append(datetime.datetime.strftime( t, '%Y-%m-%d %H:%M:%S'))\n",
    "        t += delta\n",
    "    sample = pd.DataFrame(sample_series)\n",
    "    sample =sample.rename(columns={0:\"bucket\"})\n",
    "\n",
    "    sample.insert(1, 'msg_freq',0)\n",
    "\n",
    "    merged_df = df.merge(sample, how = 'outer', on = ['bucket'])\n",
    "    new_df = merged_df.sort_values('bucket')\n",
    "    new_df = new_df.drop(columns = ['msg_freq_y']).fillna(0)\n",
    "\n",
    "    new_df  = new_df.reset_index()\n",
    "    new_df = new_df.iloc[:,1:]\n",
    "    df = new_df.rename(columns={\"msg_freq_x\": \"msg_freq\"})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cumulativeFreq(df):\n",
    "    df_cs = df.copy()\n",
    "    df_cs['bucket'] = pd.to_datetime(df_cs['bucket'])\n",
    "    df_cs[\"msg_freq\"] = df_cs[\"msg_freq\"].iloc[:].groupby([df_cs['bucket'].dt.hour, df_cs['bucket'].dt.date]).cumsum()\n",
    "    \n",
    "    return df_cs\n",
    "\n",
    "\n",
    "\n",
    "def Normalizer(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    df1 = df.iloc[:,1:]   #change required\n",
    "#     values = dff.values\n",
    "#     values = values.astype('float32')\n",
    "#     # normalize features\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaled = scaler.fit_transform(values)\n",
    "#     dff = pd.DataFrame(scaled, columns=df.columns[1:]) #change required\n",
    "    dff = df1/df1.max()\n",
    "    dff.insert(0, 'bucket', df[\"bucket\"]) #change required\n",
    "    \n",
    "    return dff\n",
    "\n",
    "\n",
    "def sigmoidDataframe(df):\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        a=0.7\n",
    "        return 1/(1+5*np.exp(-a*x))\n",
    "    \n",
    "    dff = df.copy()\n",
    "    dff['msg_freq'] = sigmoid(dff['msg_freq'])\n",
    "    return dff\n",
    "\n",
    "df = makeDataframe(df)\n",
    "df_cs = cumulativeFreq(df)\n",
    "\n",
    "dfn = Normalizer(df)\n",
    "dfn_cs = Normalizer(df_cs)\n",
    "\n",
    "\n",
    "Values = df_cs.iloc[:,1].values\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "# choose a number of time steps; basically the memory of the network\n",
    "n_steps = int(timing/bucket_size)\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(Values, n_steps)\n",
    "\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "\n",
    "def wisesplit(num,div):\n",
    "    \n",
    "    part = num - div\n",
    "    while part/num > 0.8:\n",
    "        part = part-div\n",
    "    \n",
    "    return part\n",
    "\n",
    "def print_factors(num):\n",
    "    import numpy as np\n",
    "    factors = []\n",
    "    for i in range(1, num + 1):\n",
    "        if num % i == 0:\n",
    "            factors.append(i)\n",
    "    return np.array(factors)\n",
    "\n",
    "arr =print_factors(len(X))\n",
    "div = arr[int(0.6*arr.shape[0])]\n",
    "\n",
    "split = wisesplit(len(X),div)\n",
    "# split =int(0.8*len(X))\n",
    "# split = -272\n",
    "\n",
    "train_X = np.array(X)[:split]\n",
    "train_y = np.array(y)[:split]\n",
    "test_X = np.array(X)[split:]\n",
    "test_y = np.array(y)[split:]\n",
    "\n",
    "train_y =train_y.reshape((train_y.shape[0],1))\n",
    "\n",
    "test_y = test_y.reshape((test_y.shape[0],1))\n",
    "\n",
    "def compute_hcf(x, y):\n",
    "\n",
    "# choose the smaller number\n",
    "    if x > y:\n",
    "        smaller = y\n",
    "    else:\n",
    "        smaller = x\n",
    "    for i in range(1, smaller+1):\n",
    "        if((x % i == 0) and (y % i == 0)):\n",
    "            hcf = i \n",
    "    return hcf\n",
    "\n",
    "batchsize = compute_hcf(train_X.shape[0],test_X.shape[0])\n",
    "batch_arr = print_factors(batchsize)\n",
    "bt = batch_arr[-1]\n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "    \n",
    "    \n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_unit',min_value=32,max_value=128, step =32),return_sequences=False, batch_input_shape=(bt,train_X.shape[1],train_X.shape[2]), stateful=True))\n",
    "#     for i in range(hp.Int('n_layers', 1, 4)):\n",
    "#         model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=96,step=32),return_sequences=True))\n",
    "#     model.add(LSTM(hp.Int('layer_2_neurons',min_value=32,max_value=128,step=32)))\n",
    "#     model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    hp_loss = hp.Choice('loss', values=['mean_squared_logarithmic_error'])\n",
    "    model.add(Dense(train_y.shape[1]))\n",
    "    model.compile(loss=hp_loss, optimizer=keras.optimizers.Adam(hp.Float('learning_rate', min_value=0.0001,max_value=0.001, step =0.0001)))\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='loss',\n",
    "                     max_epochs=1500,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "print(tuner.search(\n",
    "        x=train_X,\n",
    "        y=train_y,\n",
    "        epochs=1500,\n",
    "        batch_size = bt,\n",
    "        validation_data=(test_X,test_y)\n",
    "))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "print(\"The optimal number of units in the first densely-connected\", best_hps.get('input_unit'))\n",
    "print(\"The optimal learning rate for the optimizer is \",best_hps.get('learning_rate') )\n",
    "print(\"The best loss is \", best_hps.get('loss'))\n",
    "\n",
    "\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(x=train_X, y=train_y, epochs=1500, validation_data=(test_X,test_y), batch_size=batchsize, verbose =0, shuffle=False)\n",
    "\n",
    "loss_per_epoch = history.history['loss']\n",
    "best_epoch = loss_per_epoch.index(min(loss_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "final_history = hypermodel.fit(x=train_X, y=train_y, epochs=best_epoch, validation_data=(test_X,test_y), batch_size=batchsize, verbose =1, shuffle=False)\n",
    "hyper_pred= hypermodel.predict(test_X,bt)\n",
    "\n",
    "#PLOTTING\n",
    "\n",
    "plt.figure(figsize=(12,8))   \n",
    "plt.plot(final_history.history['loss'],label='Training loss')\n",
    "plt.plot(final_history.history['val_loss'], label =\"Validation loss\")\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=10)\n",
    "#             plt.ylim(0,0.2)\n",
    "plt.legend(['Training loss', 'Validation loss'])\n",
    "plt.savefig('bestmodel_loss_10_90_layer1.pdf', dpi=250)\n",
    "plt.show()\n",
    "\n",
    "print(hypermodel.summary())\n",
    "hyper_pred_df = pd.DataFrame(hyper_pred.flatten())\n",
    "test_anom = pd.DataFrame(test_y)\n",
    "\n",
    "fig = go.Figure()    \n",
    "fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= test_anom[0], name='actual'))\n",
    "# fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= tuner_pred[0], name='tuner_prediction'))\n",
    "# fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= pred_df[0], name= 'model_prediction'))\n",
    "fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= hyper_pred_df[0], name='hypermodel_prediction'))\n",
    "fig.update_layout(showlegend=True, title='Frequency v/s Time',width=1600, height=800)\n",
    "fig.update_yaxes(title_text='Normalized frequency')\n",
    "fig.update_xaxes(title_text='Time')\n",
    "fig.update_layout(xaxis_range=['2022-01-09 12:00:00','2022-01-10 00:00:00'])\n",
    "# fig.update_layout(yaxis_range=[-0.1,1])\n",
    "fig.show()\n",
    "fig.write_image(\"bestmodel_predictions_10_90_layer1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
