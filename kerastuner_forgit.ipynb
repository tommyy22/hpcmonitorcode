{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 Complete [00h 00m 03s]\n",
      "loss: 2.6948769092559814\n",
      "\n",
      "Best loss So Far: 0.8497473001480103\n",
      "Total elapsed time: 00h 01m 55s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "None\n",
      "The optimal number of units in the first densely-connected 0.0009000000000000001\n",
      "The optimal learning rate for the optimizer is  128\n",
      "The best loss is  mean_squared_logarithmic_error\n",
      "Best epoch: 1426\n",
      "Epoch 1/1426\n",
      "16/16 [==============================] - 2s 46ms/step - loss: 1.6484 - val_loss: 0.7227\n",
      "Epoch 2/1426\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.9117 - val_loss: 0.7430\n",
      "Epoch 3/1426\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8733 - val_loss: 0.7016\n",
      "Epoch 4/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8443 - val_loss: 0.6713\n",
      "Epoch 5/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8245 - val_loss: 0.6599\n",
      "Epoch 6/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8079 - val_loss: 0.6521\n",
      "Epoch 7/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7945 - val_loss: 0.6462\n",
      "Epoch 8/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7835 - val_loss: 0.6406\n",
      "Epoch 9/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7736 - val_loss: 0.6343\n",
      "Epoch 10/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7642 - val_loss: 0.6281\n",
      "Epoch 11/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7552 - val_loss: 0.6224\n",
      "Epoch 12/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7465 - val_loss: 0.6171\n",
      "Epoch 13/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7384 - val_loss: 0.6123\n",
      "Epoch 14/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7309 - val_loss: 0.6082\n",
      "Epoch 15/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7244 - val_loss: 0.6051\n",
      "Epoch 16/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7188 - val_loss: 0.6029\n",
      "Epoch 17/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7141 - val_loss: 0.6014\n",
      "Epoch 18/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7103 - val_loss: 0.6004\n",
      "Epoch 19/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7070 - val_loss: 0.5997\n",
      "Epoch 20/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7043 - val_loss: 0.5992\n",
      "Epoch 21/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7020 - val_loss: 0.5987\n",
      "Epoch 22/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6999 - val_loss: 0.5982\n",
      "Epoch 23/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6981 - val_loss: 0.5976\n",
      "Epoch 24/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6964 - val_loss: 0.5971\n",
      "Epoch 25/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6949 - val_loss: 0.5964\n",
      "Epoch 26/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6935 - val_loss: 0.5958\n",
      "Epoch 27/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6921 - val_loss: 0.5951\n",
      "Epoch 28/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6908 - val_loss: 0.5943\n",
      "Epoch 29/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6896 - val_loss: 0.5935\n",
      "Epoch 30/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6883 - val_loss: 0.5927\n",
      "Epoch 31/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6870 - val_loss: 0.5917\n",
      "Epoch 32/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6857 - val_loss: 0.5907\n",
      "Epoch 33/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6843 - val_loss: 0.5896\n",
      "Epoch 34/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6828 - val_loss: 0.5883\n",
      "Epoch 35/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6811 - val_loss: 0.5868\n",
      "Epoch 36/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6790 - val_loss: 0.5848\n",
      "Epoch 37/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6766 - val_loss: 0.5821\n",
      "Epoch 38/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6734 - val_loss: 0.5785\n",
      "Epoch 39/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6695 - val_loss: 0.5738\n",
      "Epoch 40/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6647 - val_loss: 0.5680\n",
      "Epoch 41/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6593 - val_loss: 0.5616\n",
      "Epoch 42/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6534 - val_loss: 0.5552\n",
      "Epoch 43/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6471 - val_loss: 0.5485\n",
      "Epoch 44/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6402 - val_loss: 0.5400\n",
      "Epoch 45/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6316 - val_loss: 0.5273\n",
      "Epoch 46/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6186 - val_loss: 0.5063\n",
      "Epoch 47/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5983 - val_loss: 0.4763\n",
      "Epoch 48/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.5728 - val_loss: 0.4420\n",
      "Epoch 49/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5467 - val_loss: 0.4182\n",
      "Epoch 50/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5256 - val_loss: 0.3947\n",
      "Epoch 51/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5046 - val_loss: 0.3717\n",
      "Epoch 52/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4890 - val_loss: 0.3735\n",
      "Epoch 53/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4742 - val_loss: 0.3624\n",
      "Epoch 54/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4681 - val_loss: 0.3489\n",
      "Epoch 55/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4711 - val_loss: 0.3171\n",
      "Epoch 56/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4515 - val_loss: 0.3122\n",
      "Epoch 57/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.4272 - val_loss: 0.3014\n",
      "Epoch 58/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4126 - val_loss: 0.2940\n",
      "Epoch 59/1426\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.398 - 0s 19ms/step - loss: 0.3984 - val_loss: 0.2876\n",
      "Epoch 60/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.3905 - val_loss: 0.2818\n",
      "Epoch 61/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3836 - val_loss: 0.2784\n",
      "Epoch 62/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3773 - val_loss: 0.2757\n",
      "Epoch 63/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3714 - val_loss: 0.2711\n",
      "Epoch 64/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3662 - val_loss: 0.2709\n",
      "Epoch 65/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.3662 - val_loss: 0.2694\n",
      "Epoch 66/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.3598 - val_loss: 0.2688\n",
      "Epoch 67/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.3495 - val_loss: 0.2550\n",
      "Epoch 68/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3496 - val_loss: 0.2583\n",
      "Epoch 69/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3540 - val_loss: 0.2686\n",
      "Epoch 70/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3507 - val_loss: 0.3196\n",
      "Epoch 71/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4082 - val_loss: 0.2700\n",
      "Epoch 72/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3687 - val_loss: 0.3120\n",
      "Epoch 73/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3624 - val_loss: 0.2731\n",
      "Epoch 74/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.3550 - val_loss: 0.2742\n",
      "Epoch 75/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3235 - val_loss: 0.2527\n",
      "Epoch 76/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3317 - val_loss: 0.2649\n",
      "Epoch 77/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3100 - val_loss: 0.2448\n",
      "Epoch 78/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.3150 - val_loss: 0.2603\n",
      "Epoch 79/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3061 - val_loss: 0.2440\n",
      "Epoch 80/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3031 - val_loss: 0.2533\n",
      "Epoch 81/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.3047 - val_loss: 0.2489\n",
      "Epoch 82/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2975 - val_loss: 0.2667\n",
      "Epoch 83/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3056 - val_loss: 0.2574\n",
      "Epoch 84/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2962 - val_loss: 0.2454\n",
      "Epoch 85/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2879 - val_loss: 0.2599\n",
      "Epoch 86/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2836 - val_loss: 0.2501\n",
      "Epoch 87/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2852 - val_loss: 0.2598\n",
      "Epoch 88/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2778 - val_loss: 0.2672\n",
      "Epoch 89/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.2928 - val_loss: 0.2619\n",
      "Epoch 90/1426\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3094 - val_loss: 0.3066\n",
      "Epoch 91/1426\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3180 - val_loss: 0.2845\n",
      "Epoch 92/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.3485 - val_loss: 0.3221\n",
      "Epoch 93/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2945 - val_loss: 0.2599\n",
      "Epoch 94/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2809 - val_loss: 0.2640\n",
      "Epoch 95/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2758 - val_loss: 0.2693\n",
      "Epoch 96/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2685 - val_loss: 0.2534\n",
      "Epoch 97/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2649 - val_loss: 0.2536\n",
      "Epoch 98/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2803 - val_loss: 0.2822\n",
      "Epoch 99/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2694 - val_loss: 0.2580\n",
      "Epoch 100/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2591 - val_loss: 0.2584\n",
      "Epoch 101/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2646 - val_loss: 0.2604\n",
      "Epoch 102/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2544 - val_loss: 0.2719\n",
      "Epoch 103/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.2668 - val_loss: 0.2788\n",
      "Epoch 104/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2830 - val_loss: 0.2876\n",
      "Epoch 105/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2730 - val_loss: 0.2706\n",
      "Epoch 106/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2756 - val_loss: 0.2679\n",
      "Epoch 107/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2575 - val_loss: 0.2749\n",
      "Epoch 108/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2516 - val_loss: 0.2761\n",
      "Epoch 109/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2513 - val_loss: 0.2673\n",
      "Epoch 110/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2487 - val_loss: 0.2726\n",
      "Epoch 111/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2663 - val_loss: 0.3093\n",
      "Epoch 112/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2652 - val_loss: 0.2549\n",
      "Epoch 113/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2502 - val_loss: 0.2752\n",
      "Epoch 114/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2442 - val_loss: 0.2883\n",
      "Epoch 115/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2590 - val_loss: 0.2804\n",
      "Epoch 116/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2461 - val_loss: 0.2824\n",
      "Epoch 117/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.2374 - val_loss: 0.2762\n",
      "Epoch 118/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.2312 - val_loss: 0.2869\n",
      "Epoch 119/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2276 - val_loss: 0.2735\n",
      "Epoch 120/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2215 - val_loss: 0.2966\n",
      "Epoch 121/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2244 - val_loss: 0.2786\n",
      "Epoch 122/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2245 - val_loss: 0.2911\n",
      "Epoch 123/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2186 - val_loss: 0.2856\n",
      "Epoch 124/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2239 - val_loss: 0.2710\n",
      "Epoch 125/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2284 - val_loss: 0.2812\n",
      "Epoch 126/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2759 - val_loss: 0.3060\n",
      "Epoch 127/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2682 - val_loss: 0.2787\n",
      "Epoch 128/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2748 - val_loss: 0.3128\n",
      "Epoch 129/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2616 - val_loss: 0.3081\n",
      "Epoch 130/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2520 - val_loss: 0.2708\n",
      "Epoch 131/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2255 - val_loss: 0.2809\n",
      "Epoch 132/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2172 - val_loss: 0.2730\n",
      "Epoch 133/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2035 - val_loss: 0.2774\n",
      "Epoch 134/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2076 - val_loss: 0.2847\n",
      "Epoch 135/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2011 - val_loss: 0.2957\n",
      "Epoch 136/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1964 - val_loss: 0.2864\n",
      "Epoch 137/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2016 - val_loss: 0.2869\n",
      "Epoch 138/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2188 - val_loss: 0.3019\n",
      "Epoch 139/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2147 - val_loss: 0.2914\n",
      "Epoch 140/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2182 - val_loss: 0.3036\n",
      "Epoch 141/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.2009 - val_loss: 0.2854\n",
      "Epoch 142/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1939 - val_loss: 0.2951\n",
      "Epoch 143/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1888 - val_loss: 0.2805\n",
      "Epoch 144/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1849 - val_loss: 0.2895\n",
      "Epoch 145/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.1928 - val_loss: 0.2872\n",
      "Epoch 146/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1871 - val_loss: 0.2949\n",
      "Epoch 147/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1911 - val_loss: 0.2881\n",
      "Epoch 148/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1799 - val_loss: 0.2877\n",
      "Epoch 149/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1843 - val_loss: 0.2979\n",
      "Epoch 150/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1853 - val_loss: 0.3004\n",
      "Epoch 151/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1985 - val_loss: 0.2865\n",
      "Epoch 152/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2082 - val_loss: 0.3165\n",
      "Epoch 153/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1967 - val_loss: 0.3406\n",
      "Epoch 154/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2027 - val_loss: 0.2829\n",
      "Epoch 155/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1797 - val_loss: 0.3003\n",
      "Epoch 156/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1685 - val_loss: 0.2848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1650 - val_loss: 0.3026\n",
      "Epoch 158/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1626 - val_loss: 0.2928\n",
      "Epoch 159/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1611 - val_loss: 0.2849\n",
      "Epoch 160/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1705 - val_loss: 0.3093\n",
      "Epoch 161/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1687 - val_loss: 0.3167\n",
      "Epoch 162/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1778 - val_loss: 0.2949\n",
      "Epoch 163/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1592 - val_loss: 0.3084\n",
      "Epoch 164/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1576 - val_loss: 0.3080\n",
      "Epoch 165/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1892 - val_loss: 0.3240\n",
      "Epoch 166/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1727 - val_loss: 0.2890\n",
      "Epoch 167/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1608 - val_loss: 0.3093\n",
      "Epoch 168/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1513 - val_loss: 0.3076\n",
      "Epoch 169/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1607 - val_loss: 0.3005\n",
      "Epoch 170/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1592 - val_loss: 0.3135\n",
      "Epoch 171/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1555 - val_loss: 0.3117\n",
      "Epoch 172/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1574 - val_loss: 0.3249\n",
      "Epoch 173/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1476 - val_loss: 0.3036\n",
      "Epoch 174/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1496 - val_loss: 0.3292\n",
      "Epoch 175/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1445 - val_loss: 0.3112\n",
      "Epoch 176/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.1467 - val_loss: 0.3045\n",
      "Epoch 177/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1504 - val_loss: 0.3366\n",
      "Epoch 178/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1613 - val_loss: 0.3292\n",
      "Epoch 179/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1615 - val_loss: 0.3237\n",
      "Epoch 180/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1417 - val_loss: 0.3047\n",
      "Epoch 181/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1329 - val_loss: 0.3243\n",
      "Epoch 182/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1591 - val_loss: 0.3508\n",
      "Epoch 183/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1730 - val_loss: 0.3044\n",
      "Epoch 184/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1617 - val_loss: 0.3203\n",
      "Epoch 185/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1395 - val_loss: 0.3125\n",
      "Epoch 186/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1372 - val_loss: 0.3079\n",
      "Epoch 187/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1400 - val_loss: 0.3114\n",
      "Epoch 188/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1232 - val_loss: 0.3134\n",
      "Epoch 189/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1307 - val_loss: 0.3102\n",
      "Epoch 190/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1239 - val_loss: 0.3393\n",
      "Epoch 191/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1268 - val_loss: 0.3203\n",
      "Epoch 192/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1271 - val_loss: 0.3327\n",
      "Epoch 193/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1235 - val_loss: 0.3360\n",
      "Epoch 194/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1224 - val_loss: 0.3241\n",
      "Epoch 195/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1218 - val_loss: 0.3283\n",
      "Epoch 196/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.1171 - val_loss: 0.3271\n",
      "Epoch 197/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1140 - val_loss: 0.3240\n",
      "Epoch 198/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1144 - val_loss: 0.3096\n",
      "Epoch 199/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1138 - val_loss: 0.3259\n",
      "Epoch 200/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1068 - val_loss: 0.3314\n",
      "Epoch 201/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1066 - val_loss: 0.3384\n",
      "Epoch 202/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1078 - val_loss: 0.3282\n",
      "Epoch 203/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1042 - val_loss: 0.3335\n",
      "Epoch 204/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1071 - val_loss: 0.3523\n",
      "Epoch 205/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1030 - val_loss: 0.3448\n",
      "Epoch 206/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1093 - val_loss: 0.3649\n",
      "Epoch 207/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1129 - val_loss: 0.3500\n",
      "Epoch 208/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1110 - val_loss: 0.3539\n",
      "Epoch 209/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.1038 - val_loss: 0.3237\n",
      "Epoch 210/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1118 - val_loss: 0.3624\n",
      "Epoch 211/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1092 - val_loss: 0.3375\n",
      "Epoch 212/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1124 - val_loss: 0.3377\n",
      "Epoch 213/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1215 - val_loss: 0.3558\n",
      "Epoch 214/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1185 - val_loss: 0.3620\n",
      "Epoch 215/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1154 - val_loss: 0.3601\n",
      "Epoch 216/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1035 - val_loss: 0.3289\n",
      "Epoch 217/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0949 - val_loss: 0.3645\n",
      "Epoch 218/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0909 - val_loss: 0.3450\n",
      "Epoch 219/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0838 - val_loss: 0.3406\n",
      "Epoch 220/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0809 - val_loss: 0.3608\n",
      "Epoch 221/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0792 - val_loss: 0.3570\n",
      "Epoch 222/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0780 - val_loss: 0.3565\n",
      "Epoch 223/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0779 - val_loss: 0.3621\n",
      "Epoch 224/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0820 - val_loss: 0.3596\n",
      "Epoch 225/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0814 - val_loss: 0.3705\n",
      "Epoch 226/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0913 - val_loss: 0.3946\n",
      "Epoch 227/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1237 - val_loss: 0.3452\n",
      "Epoch 228/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1491 - val_loss: 0.3926\n",
      "Epoch 229/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1321 - val_loss: 0.3669\n",
      "Epoch 230/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1368 - val_loss: 0.3722\n",
      "Epoch 231/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1114 - val_loss: 0.3299\n",
      "Epoch 232/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1094 - val_loss: 0.3811\n",
      "Epoch 233/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0910 - val_loss: 0.3496\n",
      "Epoch 234/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0875 - val_loss: 0.3655\n",
      "Epoch 235/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0809 - val_loss: 0.3620\n",
      "Epoch 236/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0807 - val_loss: 0.3603\n",
      "Epoch 237/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0793 - val_loss: 0.3770\n",
      "Epoch 238/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0810 - val_loss: 0.3662\n",
      "Epoch 239/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0748 - val_loss: 0.3635\n",
      "Epoch 240/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0727 - val_loss: 0.3678\n",
      "Epoch 241/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0755 - val_loss: 0.3749\n",
      "Epoch 242/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0736 - val_loss: 0.3807\n",
      "Epoch 243/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0830 - val_loss: 0.3473\n",
      "Epoch 244/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0840 - val_loss: 0.3805\n",
      "Epoch 245/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0795 - val_loss: 0.3947\n",
      "Epoch 246/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0763 - val_loss: 0.3633\n",
      "Epoch 247/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0745 - val_loss: 0.4013\n",
      "Epoch 248/1426\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.065 - 0s 20ms/step - loss: 0.0775 - val_loss: 0.3829\n",
      "Epoch 249/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0758 - val_loss: 0.3750\n",
      "Epoch 250/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0697 - val_loss: 0.3829\n",
      "Epoch 251/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0692 - val_loss: 0.3696\n",
      "Epoch 252/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0658 - val_loss: 0.3797\n",
      "Epoch 253/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0673 - val_loss: 0.3715\n",
      "Epoch 254/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0669 - val_loss: 0.3822\n",
      "Epoch 255/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0622 - val_loss: 0.3791\n",
      "Epoch 256/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0652 - val_loss: 0.3876\n",
      "Epoch 257/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0635 - val_loss: 0.3796\n",
      "Epoch 258/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0611 - val_loss: 0.3840\n",
      "Epoch 259/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0653 - val_loss: 0.3834\n",
      "Epoch 260/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0679 - val_loss: 0.4016\n",
      "Epoch 261/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0706 - val_loss: 0.3946\n",
      "Epoch 262/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0763 - val_loss: 0.3885\n",
      "Epoch 263/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1006 - val_loss: 0.3743\n",
      "Epoch 264/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1040 - val_loss: 0.4121\n",
      "Epoch 265/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1585 - val_loss: 0.3951\n",
      "Epoch 266/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1662 - val_loss: 0.3687\n",
      "Epoch 267/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1477 - val_loss: 0.3642\n",
      "Epoch 268/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1057 - val_loss: 0.3323\n",
      "Epoch 269/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0877 - val_loss: 0.3628\n",
      "Epoch 270/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0777 - val_loss: 0.3715\n",
      "Epoch 271/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0791 - val_loss: 0.3491\n",
      "Epoch 272/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0699 - val_loss: 0.3546\n",
      "Epoch 273/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0771 - val_loss: 0.3621\n",
      "Epoch 274/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0760 - val_loss: 0.3753\n",
      "Epoch 275/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0753 - val_loss: 0.3769\n",
      "Epoch 276/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0660 - val_loss: 0.3657\n",
      "Epoch 277/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0625 - val_loss: 0.3574\n",
      "Epoch 278/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0596 - val_loss: 0.3730\n",
      "Epoch 279/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0579 - val_loss: 0.3562\n",
      "Epoch 280/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0549 - val_loss: 0.3732\n",
      "Epoch 281/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0536 - val_loss: 0.3688\n",
      "Epoch 282/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0522 - val_loss: 0.3619\n",
      "Epoch 283/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0531 - val_loss: 0.3701\n",
      "Epoch 284/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0534 - val_loss: 0.3903\n",
      "Epoch 285/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0520 - val_loss: 0.3814\n",
      "Epoch 286/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0535 - val_loss: 0.3745\n",
      "Epoch 287/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0550 - val_loss: 0.3807\n",
      "Epoch 288/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0574 - val_loss: 0.3689\n",
      "Epoch 289/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0628 - val_loss: 0.4053\n",
      "Epoch 290/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0652 - val_loss: 0.3866\n",
      "Epoch 291/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0626 - val_loss: 0.3981\n",
      "Epoch 292/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0610 - val_loss: 0.3958\n",
      "Epoch 293/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0584 - val_loss: 0.3969\n",
      "Epoch 294/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0634 - val_loss: 0.3989\n",
      "Epoch 295/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0672 - val_loss: 0.4190\n",
      "Epoch 296/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0678 - val_loss: 0.3865\n",
      "Epoch 297/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0648 - val_loss: 0.3792\n",
      "Epoch 298/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0677 - val_loss: 0.3961\n",
      "Epoch 299/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0788 - val_loss: 0.3681\n",
      "Epoch 300/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0786 - val_loss: 0.4097\n",
      "Epoch 301/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0926 - val_loss: 0.3723\n",
      "Epoch 302/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0979 - val_loss: 0.3850\n",
      "Epoch 303/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0890 - val_loss: 0.3744\n",
      "Epoch 304/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0742 - val_loss: 0.3561\n",
      "Epoch 305/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0782 - val_loss: 0.3719\n",
      "Epoch 306/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0646 - val_loss: 0.3654\n",
      "Epoch 307/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0591 - val_loss: 0.3681\n",
      "Epoch 308/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0582 - val_loss: 0.3769\n",
      "Epoch 309/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0586 - val_loss: 0.4069\n",
      "Epoch 310/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0583 - val_loss: 0.3602\n",
      "Epoch 311/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0531 - val_loss: 0.4048\n",
      "Epoch 312/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0502 - val_loss: 0.3941\n",
      "Epoch 313/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0471 - val_loss: 0.3781\n",
      "Epoch 314/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0466 - val_loss: 0.4001\n",
      "Epoch 315/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0504 - val_loss: 0.4099\n",
      "Epoch 316/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0493 - val_loss: 0.3797\n",
      "Epoch 317/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0479 - val_loss: 0.4021\n",
      "Epoch 318/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0476 - val_loss: 0.4198\n",
      "Epoch 319/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0449 - val_loss: 0.3787\n",
      "Epoch 320/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0450 - val_loss: 0.4181\n",
      "Epoch 321/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0458 - val_loss: 0.4096\n",
      "Epoch 322/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0460 - val_loss: 0.4124\n",
      "Epoch 323/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0471 - val_loss: 0.4040\n",
      "Epoch 324/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0472 - val_loss: 0.4040\n",
      "Epoch 325/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0466 - val_loss: 0.3947\n",
      "Epoch 326/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0448 - val_loss: 0.4214\n",
      "Epoch 327/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0429 - val_loss: 0.4004\n",
      "Epoch 328/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0436 - val_loss: 0.4120\n",
      "Epoch 329/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0424 - val_loss: 0.4013\n",
      "Epoch 330/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0439 - val_loss: 0.4201\n",
      "Epoch 331/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0494 - val_loss: 0.4034\n",
      "Epoch 332/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0508 - val_loss: 0.4191\n",
      "Epoch 333/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0478 - val_loss: 0.4316\n",
      "Epoch 334/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0510 - val_loss: 0.4028\n",
      "Epoch 335/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0487 - val_loss: 0.4210\n",
      "Epoch 336/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0527 - val_loss: 0.4192\n",
      "Epoch 337/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0512 - val_loss: 0.4229\n",
      "Epoch 338/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0516 - val_loss: 0.4261\n",
      "Epoch 339/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0522 - val_loss: 0.4000\n",
      "Epoch 340/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0537 - val_loss: 0.4317\n",
      "Epoch 341/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0492 - val_loss: 0.4400\n",
      "Epoch 342/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0459 - val_loss: 0.4032\n",
      "Epoch 343/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0432 - val_loss: 0.3953\n",
      "Epoch 344/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0429 - val_loss: 0.4262\n",
      "Epoch 345/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0435 - val_loss: 0.4174\n",
      "Epoch 346/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0438 - val_loss: 0.4108\n",
      "Epoch 347/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0411 - val_loss: 0.3904\n",
      "Epoch 348/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0429 - val_loss: 0.4454\n",
      "Epoch 349/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0399 - val_loss: 0.4208\n",
      "Epoch 350/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0396 - val_loss: 0.4027\n",
      "Epoch 351/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0438 - val_loss: 0.4038\n",
      "Epoch 352/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0414 - val_loss: 0.4447\n",
      "Epoch 353/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0455 - val_loss: 0.4326\n",
      "Epoch 354/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0482 - val_loss: 0.4190\n",
      "Epoch 355/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0481 - val_loss: 0.4051\n",
      "Epoch 356/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0446 - val_loss: 0.4087\n",
      "Epoch 357/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0465 - val_loss: 0.4037\n",
      "Epoch 358/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0416 - val_loss: 0.4145\n",
      "Epoch 359/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0418 - val_loss: 0.4086\n",
      "Epoch 360/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0467 - val_loss: 0.4215\n",
      "Epoch 361/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0526 - val_loss: 0.4091\n",
      "Epoch 362/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0558 - val_loss: 0.4184\n",
      "Epoch 363/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0496 - val_loss: 0.3764\n",
      "Epoch 364/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0496 - val_loss: 0.4015\n",
      "Epoch 365/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0413 - val_loss: 0.4214\n",
      "Epoch 366/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0403 - val_loss: 0.4141\n",
      "Epoch 367/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0425 - val_loss: 0.4149\n",
      "Epoch 368/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0417 - val_loss: 0.3997\n",
      "Epoch 369/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0367 - val_loss: 0.4171\n",
      "Epoch 370/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0339 - val_loss: 0.4111\n",
      "Epoch 371/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0318 - val_loss: 0.4160\n",
      "Epoch 372/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0311 - val_loss: 0.4303\n",
      "Epoch 373/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0312 - val_loss: 0.4114\n",
      "Epoch 374/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0305 - val_loss: 0.4155\n",
      "Epoch 375/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0299 - val_loss: 0.4263\n",
      "Epoch 376/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0314 - val_loss: 0.4218\n",
      "Epoch 377/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0327 - val_loss: 0.4219\n",
      "Epoch 378/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0362 - val_loss: 0.4263\n",
      "Epoch 379/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0354 - val_loss: 0.4071\n",
      "Epoch 380/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0378 - val_loss: 0.4187\n",
      "Epoch 381/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0378 - val_loss: 0.4123\n",
      "Epoch 382/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0398 - val_loss: 0.4067\n",
      "Epoch 383/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0418 - val_loss: 0.3989\n",
      "Epoch 384/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0430 - val_loss: 0.4043\n",
      "Epoch 385/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0465 - val_loss: 0.4055\n",
      "Epoch 386/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0450 - val_loss: 0.4172\n",
      "Epoch 387/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0402 - val_loss: 0.3978\n",
      "Epoch 388/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0386 - val_loss: 0.4052\n",
      "Epoch 389/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0364 - val_loss: 0.4160\n",
      "Epoch 390/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0360 - val_loss: 0.4121\n",
      "Epoch 391/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0334 - val_loss: 0.4085\n",
      "Epoch 392/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0319 - val_loss: 0.4216\n",
      "Epoch 393/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0344 - val_loss: 0.4329\n",
      "Epoch 394/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0345 - val_loss: 0.4021\n",
      "Epoch 395/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0362 - val_loss: 0.4191\n",
      "Epoch 396/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0393 - val_loss: 0.4157\n",
      "Epoch 397/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0391 - val_loss: 0.4208\n",
      "Epoch 398/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0347 - val_loss: 0.4374\n",
      "Epoch 399/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0323 - val_loss: 0.4248\n",
      "Epoch 400/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0323 - val_loss: 0.4301\n",
      "Epoch 401/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0342 - val_loss: 0.4247\n",
      "Epoch 402/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0361 - val_loss: 0.4213\n",
      "Epoch 403/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0413 - val_loss: 0.3926\n",
      "Epoch 404/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0448 - val_loss: 0.4092\n",
      "Epoch 405/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0383 - val_loss: 0.4474\n",
      "Epoch 406/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0399 - val_loss: 0.4118\n",
      "Epoch 407/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0429 - val_loss: 0.4445\n",
      "Epoch 408/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0513 - val_loss: 0.4191\n",
      "Epoch 409/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0638 - val_loss: 0.4186\n",
      "Epoch 410/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0510 - val_loss: 0.4416\n",
      "Epoch 411/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0495 - val_loss: 0.3788\n",
      "Epoch 412/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0433 - val_loss: 0.4479\n",
      "Epoch 413/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0401 - val_loss: 0.3807\n",
      "Epoch 414/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0423 - val_loss: 0.4718\n",
      "Epoch 415/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0396 - val_loss: 0.4174\n",
      "Epoch 416/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0407 - val_loss: 0.4174\n",
      "Epoch 417/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0430 - val_loss: 0.3667\n",
      "Epoch 418/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0431 - val_loss: 0.4445\n",
      "Epoch 419/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0412 - val_loss: 0.4187\n",
      "Epoch 420/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0394 - val_loss: 0.4257\n",
      "Epoch 421/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0382 - val_loss: 0.4174\n",
      "Epoch 422/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0500 - val_loss: 0.4137\n",
      "Epoch 423/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0491 - val_loss: 0.4108\n",
      "Epoch 424/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0424 - val_loss: 0.3967\n",
      "Epoch 425/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0375 - val_loss: 0.4170\n",
      "Epoch 426/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0326 - val_loss: 0.4186\n",
      "Epoch 427/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0304 - val_loss: 0.3893\n",
      "Epoch 428/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0297 - val_loss: 0.3828\n",
      "Epoch 429/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0307 - val_loss: 0.4220\n",
      "Epoch 430/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0304 - val_loss: 0.4199\n",
      "Epoch 431/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0300 - val_loss: 0.3950\n",
      "Epoch 432/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0292 - val_loss: 0.4237\n",
      "Epoch 433/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0308 - val_loss: 0.4081\n",
      "Epoch 434/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0300 - val_loss: 0.3929\n",
      "Epoch 435/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0291 - val_loss: 0.4096\n",
      "Epoch 436/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0273 - val_loss: 0.4007\n",
      "Epoch 437/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0254 - val_loss: 0.3871\n",
      "Epoch 438/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0249 - val_loss: 0.4201\n",
      "Epoch 439/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0256 - val_loss: 0.4224\n",
      "Epoch 440/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0260 - val_loss: 0.4117\n",
      "Epoch 441/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0247 - val_loss: 0.4038\n",
      "Epoch 442/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0241 - val_loss: 0.4310\n",
      "Epoch 443/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0247 - val_loss: 0.4011\n",
      "Epoch 444/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0234 - val_loss: 0.4260\n",
      "Epoch 445/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0238 - val_loss: 0.4229\n",
      "Epoch 446/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0230 - val_loss: 0.4136\n",
      "Epoch 447/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0242 - val_loss: 0.4133\n",
      "Epoch 448/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0250 - val_loss: 0.4077\n",
      "Epoch 449/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0233 - val_loss: 0.4041\n",
      "Epoch 450/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0245 - val_loss: 0.4102\n",
      "Epoch 451/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0218 - val_loss: 0.4159\n",
      "Epoch 452/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0196 - val_loss: 0.4092\n",
      "Epoch 453/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0202 - val_loss: 0.4027\n",
      "Epoch 454/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0208 - val_loss: 0.4218\n",
      "Epoch 455/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0214 - val_loss: 0.4144\n",
      "Epoch 456/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0247 - val_loss: 0.4459\n",
      "Epoch 457/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0289 - val_loss: 0.4427\n",
      "Epoch 458/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0300 - val_loss: 0.4229\n",
      "Epoch 459/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0356 - val_loss: 0.4215\n",
      "Epoch 460/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0345 - val_loss: 0.4084\n",
      "Epoch 461/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0402 - val_loss: 0.4365\n",
      "Epoch 462/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0403 - val_loss: 0.3744\n",
      "Epoch 463/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0439 - val_loss: 0.4200\n",
      "Epoch 464/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0381 - val_loss: 0.4154\n",
      "Epoch 465/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0365 - val_loss: 0.4244\n",
      "Epoch 466/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0322 - val_loss: 0.4167\n",
      "Epoch 467/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0397 - val_loss: 0.3936\n",
      "Epoch 468/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0325 - val_loss: 0.4109\n",
      "Epoch 469/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0289 - val_loss: 0.4304\n",
      "Epoch 470/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0270 - val_loss: 0.4143\n",
      "Epoch 471/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0254 - val_loss: 0.4204\n",
      "Epoch 472/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0273 - val_loss: 0.4286\n",
      "Epoch 473/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0261 - val_loss: 0.3883\n",
      "Epoch 474/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0229 - val_loss: 0.4075\n",
      "Epoch 475/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0212 - val_loss: 0.4047\n",
      "Epoch 476/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0208 - val_loss: 0.3984\n",
      "Epoch 477/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0190 - val_loss: 0.4188\n",
      "Epoch 478/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0183 - val_loss: 0.4035\n",
      "Epoch 479/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0186 - val_loss: 0.4028\n",
      "Epoch 480/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0174 - val_loss: 0.4079\n",
      "Epoch 481/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0190 - val_loss: 0.4266\n",
      "Epoch 482/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0173 - val_loss: 0.4226\n",
      "Epoch 483/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0177 - val_loss: 0.4124\n",
      "Epoch 484/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0181 - val_loss: 0.4086\n",
      "Epoch 485/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0176 - val_loss: 0.4206\n",
      "Epoch 486/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0177 - val_loss: 0.4077\n",
      "Epoch 487/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0190 - val_loss: 0.4323\n",
      "Epoch 488/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0184 - val_loss: 0.4300\n",
      "Epoch 489/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0189 - val_loss: 0.4270\n",
      "Epoch 490/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0207 - val_loss: 0.3997\n",
      "Epoch 491/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0205 - val_loss: 0.4255\n",
      "Epoch 492/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0259 - val_loss: 0.4336\n",
      "Epoch 493/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0343 - val_loss: 0.4695\n",
      "Epoch 494/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0339 - val_loss: 0.4037\n",
      "Epoch 495/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0391 - val_loss: 0.4351\n",
      "Epoch 496/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0459 - val_loss: 0.3966\n",
      "Epoch 497/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0373 - val_loss: 0.4039\n",
      "Epoch 498/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0424 - val_loss: 0.4481\n",
      "Epoch 499/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0477 - val_loss: 0.4250\n",
      "Epoch 500/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0416 - val_loss: 0.4347\n",
      "Epoch 501/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0349 - val_loss: 0.4342\n",
      "Epoch 502/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0310 - val_loss: 0.4314\n",
      "Epoch 503/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0261 - val_loss: 0.4406\n",
      "Epoch 504/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0259 - val_loss: 0.3970\n",
      "Epoch 505/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0310 - val_loss: 0.3960\n",
      "Epoch 506/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0299 - val_loss: 0.4311\n",
      "Epoch 507/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0291 - val_loss: 0.3764\n",
      "Epoch 508/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0319 - val_loss: 0.4327\n",
      "Epoch 509/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0309 - val_loss: 0.4074\n",
      "Epoch 510/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0340 - val_loss: 0.4261\n",
      "Epoch 511/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0249 - val_loss: 0.4091\n",
      "Epoch 512/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0248 - val_loss: 0.4346\n",
      "Epoch 513/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0223 - val_loss: 0.4224\n",
      "Epoch 514/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0231 - val_loss: 0.4187\n",
      "Epoch 515/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0182 - val_loss: 0.4142\n",
      "Epoch 516/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0169 - val_loss: 0.4106\n",
      "Epoch 517/1426\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0154 - val_loss: 0.4046\n",
      "Epoch 518/1426\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.0153 - val_loss: 0.4211\n",
      "Epoch 519/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0141 - val_loss: 0.4208\n",
      "Epoch 520/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0140 - val_loss: 0.4140\n",
      "Epoch 521/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0131 - val_loss: 0.4178\n",
      "Epoch 522/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0133 - val_loss: 0.4277\n",
      "Epoch 523/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0129 - val_loss: 0.4263\n",
      "Epoch 524/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0129 - val_loss: 0.4266\n",
      "Epoch 525/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0125 - val_loss: 0.4257\n",
      "Epoch 526/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0131 - val_loss: 0.4318\n",
      "Epoch 527/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0131 - val_loss: 0.4306\n",
      "Epoch 528/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0144 - val_loss: 0.4353\n",
      "Epoch 529/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0207 - val_loss: 0.4026\n",
      "Epoch 530/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0168 - val_loss: 0.4334\n",
      "Epoch 531/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0174 - val_loss: 0.4232\n",
      "Epoch 532/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0198 - val_loss: 0.4302\n",
      "Epoch 533/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0178 - val_loss: 0.4233\n",
      "Epoch 534/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0218 - val_loss: 0.4335\n",
      "Epoch 535/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0240 - val_loss: 0.4385\n",
      "Epoch 536/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0230 - val_loss: 0.4380\n",
      "Epoch 537/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0295 - val_loss: 0.4274\n",
      "Epoch 538/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0449 - val_loss: 0.4264\n",
      "Epoch 539/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0398 - val_loss: 0.4058\n",
      "Epoch 540/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0390 - val_loss: 0.4071\n",
      "Epoch 541/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0312 - val_loss: 0.4159\n",
      "Epoch 542/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0285 - val_loss: 0.4254\n",
      "Epoch 543/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0263 - val_loss: 0.4167\n",
      "Epoch 544/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0239 - val_loss: 0.4258\n",
      "Epoch 545/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0213 - val_loss: 0.4205\n",
      "Epoch 546/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0227 - val_loss: 0.4307\n",
      "Epoch 547/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0191 - val_loss: 0.4276\n",
      "Epoch 548/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0179 - val_loss: 0.4170\n",
      "Epoch 549/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0159 - val_loss: 0.4438\n",
      "Epoch 550/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0153 - val_loss: 0.4252\n",
      "Epoch 551/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0167 - val_loss: 0.4133\n",
      "Epoch 552/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0221 - val_loss: 0.4280\n",
      "Epoch 553/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0181 - val_loss: 0.4123\n",
      "Epoch 554/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0175 - val_loss: 0.4089\n",
      "Epoch 555/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0199 - val_loss: 0.3918\n",
      "Epoch 556/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0276 - val_loss: 0.4282\n",
      "Epoch 557/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0189 - val_loss: 0.4141\n",
      "Epoch 558/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0234 - val_loss: 0.4005\n",
      "Epoch 559/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0186 - val_loss: 0.4222\n",
      "Epoch 560/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0240 - val_loss: 0.4291\n",
      "Epoch 561/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0167 - val_loss: 0.4173\n",
      "Epoch 562/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0149 - val_loss: 0.4209\n",
      "Epoch 563/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0148 - val_loss: 0.4223\n",
      "Epoch 564/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0141 - val_loss: 0.4298\n",
      "Epoch 565/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0139 - val_loss: 0.4228\n",
      "Epoch 566/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0162 - val_loss: 0.4590\n",
      "Epoch 567/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0253 - val_loss: 0.4152\n",
      "Epoch 568/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0198 - val_loss: 0.4717\n",
      "Epoch 569/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0248 - val_loss: 0.4142\n",
      "Epoch 570/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0263 - val_loss: 0.4490\n",
      "Epoch 571/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0312 - val_loss: 0.4205\n",
      "Epoch 572/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0461 - val_loss: 0.4419\n",
      "Epoch 573/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0591 - val_loss: 0.3913\n",
      "Epoch 574/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0864 - val_loss: 0.4027\n",
      "Epoch 575/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0890 - val_loss: 0.4006\n",
      "Epoch 576/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0929 - val_loss: 0.4592\n",
      "Epoch 577/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0738 - val_loss: 0.4086\n",
      "Epoch 578/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0829 - val_loss: 0.4135\n",
      "Epoch 579/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1000 - val_loss: 0.4323\n",
      "Epoch 580/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0893 - val_loss: 0.3725\n",
      "Epoch 581/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0620 - val_loss: 0.3936\n",
      "Epoch 582/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0505 - val_loss: 0.4018\n",
      "Epoch 583/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0370 - val_loss: 0.3850\n",
      "Epoch 584/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0318 - val_loss: 0.3932\n",
      "Epoch 585/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0257 - val_loss: 0.4002\n",
      "Epoch 586/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0228 - val_loss: 0.3882\n",
      "Epoch 587/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0184 - val_loss: 0.3885\n",
      "Epoch 588/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0170 - val_loss: 0.3999\n",
      "Epoch 589/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0153 - val_loss: 0.3825\n",
      "Epoch 590/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0145 - val_loss: 0.3949\n",
      "Epoch 591/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0133 - val_loss: 0.3965\n",
      "Epoch 592/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0124 - val_loss: 0.3964\n",
      "Epoch 593/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0119 - val_loss: 0.3992\n",
      "Epoch 594/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0115 - val_loss: 0.4016\n",
      "Epoch 595/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0113 - val_loss: 0.4029\n",
      "Epoch 596/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0110 - val_loss: 0.4067\n",
      "Epoch 597/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0107 - val_loss: 0.4110\n",
      "Epoch 598/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0105 - val_loss: 0.4120\n",
      "Epoch 599/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0103 - val_loss: 0.4161\n",
      "Epoch 600/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0100 - val_loss: 0.4169\n",
      "Epoch 601/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0099 - val_loss: 0.4205\n",
      "Epoch 602/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0099 - val_loss: 0.4181\n",
      "Epoch 603/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0097 - val_loss: 0.4218\n",
      "Epoch 604/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0095 - val_loss: 0.4216\n",
      "Epoch 605/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0095 - val_loss: 0.4219\n",
      "Epoch 606/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0111 - val_loss: 0.4175\n",
      "Epoch 607/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0108 - val_loss: 0.4115\n",
      "Epoch 608/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.4127\n",
      "Epoch 609/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.4180\n",
      "Epoch 610/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.4187\n",
      "Epoch 611/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.4179\n",
      "Epoch 612/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0099 - val_loss: 0.4220\n",
      "Epoch 613/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0096 - val_loss: 0.4230\n",
      "Epoch 614/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0092 - val_loss: 0.4228\n",
      "Epoch 615/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.4187\n",
      "Epoch 616/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0089 - val_loss: 0.4301\n",
      "Epoch 617/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0088 - val_loss: 0.4192\n",
      "Epoch 618/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0091 - val_loss: 0.4311\n",
      "Epoch 619/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0095 - val_loss: 0.4167\n",
      "Epoch 620/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0099 - val_loss: 0.4331\n",
      "Epoch 621/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0096 - val_loss: 0.4175\n",
      "Epoch 622/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0097 - val_loss: 0.4236\n",
      "Epoch 623/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0156 - val_loss: 0.4056\n",
      "Epoch 624/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0172 - val_loss: 0.4056\n",
      "Epoch 625/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0178 - val_loss: 0.4201\n",
      "Epoch 626/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0164 - val_loss: 0.4399\n",
      "Epoch 627/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0179 - val_loss: 0.4268\n",
      "Epoch 628/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0159 - val_loss: 0.4189\n",
      "Epoch 629/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0149 - val_loss: 0.3861\n",
      "Epoch 630/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0175 - val_loss: 0.4277\n",
      "Epoch 631/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0143 - val_loss: 0.4093\n",
      "Epoch 632/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0127 - val_loss: 0.4094\n",
      "Epoch 633/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0123 - val_loss: 0.4153\n",
      "Epoch 634/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0122 - val_loss: 0.3923\n",
      "Epoch 635/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0144 - val_loss: 0.4220\n",
      "Epoch 636/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0140 - val_loss: 0.4255\n",
      "Epoch 637/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0141 - val_loss: 0.3993\n",
      "Epoch 638/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0130 - val_loss: 0.4320\n",
      "Epoch 639/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0131 - val_loss: 0.4007\n",
      "Epoch 640/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0127 - val_loss: 0.4008\n",
      "Epoch 641/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0125 - val_loss: 0.3993\n",
      "Epoch 642/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0109 - val_loss: 0.4200\n",
      "Epoch 643/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0117 - val_loss: 0.3975\n",
      "Epoch 644/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0107 - val_loss: 0.4009\n",
      "Epoch 645/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0101 - val_loss: 0.3912\n",
      "Epoch 646/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0100 - val_loss: 0.4101\n",
      "Epoch 647/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.3996\n",
      "Epoch 648/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0113 - val_loss: 0.4210\n",
      "Epoch 649/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0115 - val_loss: 0.3991\n",
      "Epoch 650/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0150 - val_loss: 0.3902\n",
      "Epoch 651/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0113 - val_loss: 0.4098\n",
      "Epoch 652/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0106 - val_loss: 0.4063\n",
      "Epoch 653/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0110 - val_loss: 0.4096\n",
      "Epoch 654/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0118 - val_loss: 0.4243\n",
      "Epoch 655/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0131 - val_loss: 0.3976\n",
      "Epoch 656/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0126 - val_loss: 0.4068\n",
      "Epoch 657/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0142 - val_loss: 0.4176\n",
      "Epoch 658/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0148 - val_loss: 0.4023\n",
      "Epoch 659/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0145 - val_loss: 0.4187\n",
      "Epoch 660/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0158 - val_loss: 0.4016\n",
      "Epoch 661/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0190 - val_loss: 0.4044\n",
      "Epoch 662/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0230 - val_loss: 0.4225\n",
      "Epoch 663/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0236 - val_loss: 0.3996\n",
      "Epoch 664/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0181 - val_loss: 0.4207\n",
      "Epoch 665/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0184 - val_loss: 0.4187\n",
      "Epoch 666/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0207 - val_loss: 0.4236\n",
      "Epoch 667/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0217 - val_loss: 0.3866\n",
      "Epoch 668/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0238 - val_loss: 0.4177\n",
      "Epoch 669/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0245 - val_loss: 0.3831\n",
      "Epoch 670/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0334 - val_loss: 0.4299\n",
      "Epoch 671/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0302 - val_loss: 0.3924\n",
      "Epoch 672/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0287 - val_loss: 0.4378\n",
      "Epoch 673/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0408 - val_loss: 0.4133\n",
      "Epoch 674/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0666 - val_loss: 0.4415\n",
      "Epoch 675/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0461 - val_loss: 0.4279\n",
      "Epoch 676/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0403 - val_loss: 0.4109\n",
      "Epoch 677/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0263 - val_loss: 0.3791\n",
      "Epoch 678/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0220 - val_loss: 0.3866\n",
      "Epoch 679/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0156 - val_loss: 0.4008\n",
      "Epoch 680/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0137 - val_loss: 0.3850\n",
      "Epoch 681/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0119 - val_loss: 0.3837\n",
      "Epoch 682/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0098 - val_loss: 0.3832\n",
      "Epoch 683/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0089 - val_loss: 0.3949\n",
      "Epoch 684/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0091 - val_loss: 0.3875\n",
      "Epoch 685/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0091 - val_loss: 0.3846\n",
      "Epoch 686/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0085 - val_loss: 0.3934\n",
      "Epoch 687/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0081 - val_loss: 0.3895\n",
      "Epoch 688/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0085 - val_loss: 0.3820\n",
      "Epoch 689/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0088 - val_loss: 0.3927\n",
      "Epoch 690/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0083 - val_loss: 0.3962\n",
      "Epoch 691/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0079 - val_loss: 0.3879\n",
      "Epoch 692/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0080 - val_loss: 0.3889\n",
      "Epoch 693/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0089 - val_loss: 0.3923\n",
      "Epoch 694/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0080 - val_loss: 0.3946\n",
      "Epoch 695/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0079 - val_loss: 0.3915\n",
      "Epoch 696/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0079 - val_loss: 0.3842\n",
      "Epoch 697/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0084 - val_loss: 0.3955\n",
      "Epoch 698/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0076 - val_loss: 0.4010\n",
      "Epoch 699/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0079 - val_loss: 0.3962\n",
      "Epoch 700/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0075 - val_loss: 0.3857\n",
      "Epoch 701/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0087 - val_loss: 0.3969\n",
      "Epoch 702/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0080 - val_loss: 0.4087\n",
      "Epoch 703/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0085 - val_loss: 0.4030\n",
      "Epoch 704/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0080 - val_loss: 0.3904\n",
      "Epoch 705/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0090 - val_loss: 0.3984\n",
      "Epoch 706/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0078 - val_loss: 0.4040\n",
      "Epoch 707/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0078 - val_loss: 0.3984\n",
      "Epoch 708/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0074 - val_loss: 0.3892\n",
      "Epoch 709/1426\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.007 - 0s 20ms/step - loss: 0.0079 - val_loss: 0.3968\n",
      "Epoch 710/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0073 - val_loss: 0.3999\n",
      "Epoch 711/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0073 - val_loss: 0.4006\n",
      "Epoch 712/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0074 - val_loss: 0.3956\n",
      "Epoch 713/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0074 - val_loss: 0.3979\n",
      "Epoch 714/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0070 - val_loss: 0.4034\n",
      "Epoch 715/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0065 - val_loss: 0.3998\n",
      "Epoch 716/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0068 - val_loss: 0.3974\n",
      "Epoch 717/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0091 - val_loss: 0.3999\n",
      "Epoch 718/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0126 - val_loss: 0.4017\n",
      "Epoch 719/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0162 - val_loss: 0.4148\n",
      "Epoch 720/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0170 - val_loss: 0.4062\n",
      "Epoch 721/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0176 - val_loss: 0.4102\n",
      "Epoch 722/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0174 - val_loss: 0.4144\n",
      "Epoch 723/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0268 - val_loss: 0.4354\n",
      "Epoch 724/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0322 - val_loss: 0.4096\n",
      "Epoch 725/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0277 - val_loss: 0.4503\n",
      "Epoch 726/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0285 - val_loss: 0.4055\n",
      "Epoch 727/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0259 - val_loss: 0.4036\n",
      "Epoch 728/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0289 - val_loss: 0.3907\n",
      "Epoch 729/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0366 - val_loss: 0.4267\n",
      "Epoch 730/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0300 - val_loss: 0.4078\n",
      "Epoch 731/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0391 - val_loss: 0.4076\n",
      "Epoch 732/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0323 - val_loss: 0.4136\n",
      "Epoch 733/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0291 - val_loss: 0.3996\n",
      "Epoch 734/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0188 - val_loss: 0.3874\n",
      "Epoch 735/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0177 - val_loss: 0.4252\n",
      "Epoch 736/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0153 - val_loss: 0.3912\n",
      "Epoch 737/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0132 - val_loss: 0.4025\n",
      "Epoch 738/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0105 - val_loss: 0.3943\n",
      "Epoch 739/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.3771\n",
      "Epoch 740/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0084 - val_loss: 0.3875\n",
      "Epoch 741/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0070 - val_loss: 0.3861\n",
      "Epoch 742/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0064 - val_loss: 0.3878\n",
      "Epoch 743/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0063 - val_loss: 0.3886\n",
      "Epoch 744/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0062 - val_loss: 0.3850\n",
      "Epoch 745/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0066 - val_loss: 0.3859\n",
      "Epoch 746/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0062 - val_loss: 0.3937\n",
      "Epoch 747/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0060 - val_loss: 0.3921\n",
      "Epoch 748/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0056 - val_loss: 0.3863\n",
      "Epoch 749/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0053 - val_loss: 0.3905\n",
      "Epoch 750/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0054 - val_loss: 0.3991\n",
      "Epoch 751/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0054 - val_loss: 0.3942\n",
      "Epoch 752/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0054 - val_loss: 0.3857\n",
      "Epoch 753/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0052 - val_loss: 0.4045\n",
      "Epoch 754/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0051 - val_loss: 0.3989\n",
      "Epoch 755/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0056 - val_loss: 0.3948\n",
      "Epoch 756/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0057 - val_loss: 0.3935\n",
      "Epoch 757/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0058 - val_loss: 0.4086\n",
      "Epoch 758/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.4012\n",
      "Epoch 759/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.3856\n",
      "Epoch 760/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0061 - val_loss: 0.4043\n",
      "Epoch 761/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0056 - val_loss: 0.4038\n",
      "Epoch 762/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0061 - val_loss: 0.4045\n",
      "Epoch 763/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0065 - val_loss: 0.3950\n",
      "Epoch 764/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0077 - val_loss: 0.4195\n",
      "Epoch 765/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0081 - val_loss: 0.3927\n",
      "Epoch 766/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0088 - val_loss: 0.4014\n",
      "Epoch 767/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0091 - val_loss: 0.4048\n",
      "Epoch 768/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0086 - val_loss: 0.4146\n",
      "Epoch 769/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0103 - val_loss: 0.3838\n",
      "Epoch 770/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0102 - val_loss: 0.4029\n",
      "Epoch 771/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0108 - val_loss: 0.3900\n",
      "Epoch 772/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0109 - val_loss: 0.4171\n",
      "Epoch 773/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.3767\n",
      "Epoch 774/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0090 - val_loss: 0.4060\n",
      "Epoch 775/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0101 - val_loss: 0.3874\n",
      "Epoch 776/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0089 - val_loss: 0.3972\n",
      "Epoch 777/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0081 - val_loss: 0.3894\n",
      "Epoch 778/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0072 - val_loss: 0.3946\n",
      "Epoch 779/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0071 - val_loss: 0.3914\n",
      "Epoch 780/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0067 - val_loss: 0.4045\n",
      "Epoch 781/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0060 - val_loss: 0.3818\n",
      "Epoch 782/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0060 - val_loss: 0.4215\n",
      "Epoch 783/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0059 - val_loss: 0.3931\n",
      "Epoch 784/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0061 - val_loss: 0.4018\n",
      "Epoch 785/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0056 - val_loss: 0.3866\n",
      "Epoch 786/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0063 - val_loss: 0.4144\n",
      "Epoch 787/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0060 - val_loss: 0.4083\n",
      "Epoch 788/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0071 - val_loss: 0.3946\n",
      "Epoch 789/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0087 - val_loss: 0.3937\n",
      "Epoch 790/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0077 - val_loss: 0.4174\n",
      "Epoch 791/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0076 - val_loss: 0.4010\n",
      "Epoch 792/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0082 - val_loss: 0.4064\n",
      "Epoch 793/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0080 - val_loss: 0.3876\n",
      "Epoch 794/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0080 - val_loss: 0.4214\n",
      "Epoch 795/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0139 - val_loss: 0.3885\n",
      "Epoch 796/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0146 - val_loss: 0.3994\n",
      "Epoch 797/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0180 - val_loss: 0.4190\n",
      "Epoch 798/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0206 - val_loss: 0.3872\n",
      "Epoch 799/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0183 - val_loss: 0.3848\n",
      "Epoch 800/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0152 - val_loss: 0.3848\n",
      "Epoch 801/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0136 - val_loss: 0.4008\n",
      "Epoch 802/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0134 - val_loss: 0.3874\n",
      "Epoch 803/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0113 - val_loss: 0.4010\n",
      "Epoch 804/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0088 - val_loss: 0.3927\n",
      "Epoch 805/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0087 - val_loss: 0.4082\n",
      "Epoch 806/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0083 - val_loss: 0.3812\n",
      "Epoch 807/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0086 - val_loss: 0.4039\n",
      "Epoch 808/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0080 - val_loss: 0.3978\n",
      "Epoch 809/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0095 - val_loss: 0.3819\n",
      "Epoch 810/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0095 - val_loss: 0.3881\n",
      "Epoch 811/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.4154\n",
      "Epoch 812/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0092 - val_loss: 0.3983\n",
      "Epoch 813/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0111 - val_loss: 0.3932\n",
      "Epoch 814/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0100 - val_loss: 0.4082\n",
      "Epoch 815/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0100 - val_loss: 0.4116\n",
      "Epoch 816/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0105 - val_loss: 0.3892\n",
      "Epoch 817/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0147 - val_loss: 0.3889\n",
      "Epoch 818/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0137 - val_loss: 0.4070\n",
      "Epoch 819/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0132 - val_loss: 0.3951\n",
      "Epoch 820/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0141 - val_loss: 0.3935\n",
      "Epoch 821/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0129 - val_loss: 0.4145\n",
      "Epoch 822/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0110 - val_loss: 0.3850\n",
      "Epoch 823/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0134 - val_loss: 0.3912\n",
      "Epoch 824/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0144 - val_loss: 0.3983\n",
      "Epoch 825/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0122 - val_loss: 0.3891\n",
      "Epoch 826/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0104 - val_loss: 0.3852\n",
      "Epoch 827/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0103 - val_loss: 0.3904\n",
      "Epoch 828/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0080 - val_loss: 0.3893\n",
      "Epoch 829/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0071 - val_loss: 0.3822\n",
      "Epoch 830/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0057 - val_loss: 0.3771\n",
      "Epoch 831/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0049 - val_loss: 0.3837\n",
      "Epoch 832/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0046 - val_loss: 0.3907\n",
      "Epoch 833/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0050 - val_loss: 0.3798\n",
      "Epoch 834/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0044 - val_loss: 0.4004\n",
      "Epoch 835/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0043 - val_loss: 0.3929\n",
      "Epoch 836/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0045 - val_loss: 0.3822\n",
      "Epoch 837/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0048 - val_loss: 0.3938\n",
      "Epoch 838/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0051 - val_loss: 0.4013\n",
      "Epoch 839/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0048 - val_loss: 0.3868\n",
      "Epoch 840/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.3801\n",
      "Epoch 841/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0050 - val_loss: 0.3944\n",
      "Epoch 842/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0064 - val_loss: 0.4055\n",
      "Epoch 843/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0071 - val_loss: 0.3911\n",
      "Epoch 844/1426\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.0085 - val_loss: 0.3905\n",
      "Epoch 845/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0070 - val_loss: 0.4073\n",
      "Epoch 846/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0074 - val_loss: 0.3889\n",
      "Epoch 847/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0069 - val_loss: 0.3964\n",
      "Epoch 848/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0068 - val_loss: 0.3763\n",
      "Epoch 849/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0067 - val_loss: 0.4201\n",
      "Epoch 850/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0071 - val_loss: 0.4005\n",
      "Epoch 851/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0083 - val_loss: 0.4127\n",
      "Epoch 852/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0093 - val_loss: 0.4030\n",
      "Epoch 853/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0097 - val_loss: 0.4095\n",
      "Epoch 854/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0107 - val_loss: 0.4077\n",
      "Epoch 855/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0097 - val_loss: 0.4048\n",
      "Epoch 856/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0094 - val_loss: 0.3951\n",
      "Epoch 857/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0107 - val_loss: 0.3923\n",
      "Epoch 858/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0105 - val_loss: 0.4106\n",
      "Epoch 859/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0122 - val_loss: 0.3964\n",
      "Epoch 860/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0112 - val_loss: 0.3853\n",
      "Epoch 861/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0118 - val_loss: 0.3928\n",
      "Epoch 862/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0108 - val_loss: 0.3827\n",
      "Epoch 863/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0111 - val_loss: 0.3998\n",
      "Epoch 864/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0099 - val_loss: 0.3862\n",
      "Epoch 865/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0095 - val_loss: 0.3998\n",
      "Epoch 866/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0083 - val_loss: 0.3806\n",
      "Epoch 867/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0075 - val_loss: 0.3952\n",
      "Epoch 868/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0059 - val_loss: 0.4038\n",
      "Epoch 869/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0060 - val_loss: 0.3942\n",
      "Epoch 870/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0051 - val_loss: 0.3814\n",
      "Epoch 871/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0045 - val_loss: 0.4055\n",
      "Epoch 872/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0036 - val_loss: 0.3873\n",
      "Epoch 873/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0033 - val_loss: 0.3945\n",
      "Epoch 874/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0032 - val_loss: 0.3898\n",
      "Epoch 875/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0032 - val_loss: 0.3976\n",
      "Epoch 876/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0032 - val_loss: 0.3930\n",
      "Epoch 877/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0031 - val_loss: 0.3956\n",
      "Epoch 878/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0033 - val_loss: 0.3950\n",
      "Epoch 879/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0034 - val_loss: 0.3945\n",
      "Epoch 880/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0036 - val_loss: 0.3988\n",
      "Epoch 881/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0037 - val_loss: 0.3997\n",
      "Epoch 882/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0040 - val_loss: 0.4002\n",
      "Epoch 883/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.3946\n",
      "Epoch 884/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0040 - val_loss: 0.4000\n",
      "Epoch 885/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0040 - val_loss: 0.4009\n",
      "Epoch 886/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0046 - val_loss: 0.4026\n",
      "Epoch 887/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0046 - val_loss: 0.3907\n",
      "Epoch 888/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0048 - val_loss: 0.4009\n",
      "Epoch 889/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0046 - val_loss: 0.4013\n",
      "Epoch 890/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0057 - val_loss: 0.4004\n",
      "Epoch 891/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0059 - val_loss: 0.3876\n",
      "Epoch 892/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0059 - val_loss: 0.3981\n",
      "Epoch 893/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.3949\n",
      "Epoch 894/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0060 - val_loss: 0.4109\n",
      "Epoch 895/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0067 - val_loss: 0.3836\n",
      "Epoch 896/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0067 - val_loss: 0.4051\n",
      "Epoch 897/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0061 - val_loss: 0.3912\n",
      "Epoch 898/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0057 - val_loss: 0.4050\n",
      "Epoch 899/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0069 - val_loss: 0.3848\n",
      "Epoch 900/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0068 - val_loss: 0.4058\n",
      "Epoch 901/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0064 - val_loss: 0.3832\n",
      "Epoch 902/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0056 - val_loss: 0.4051\n",
      "Epoch 903/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0060 - val_loss: 0.4023\n",
      "Epoch 904/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0080 - val_loss: 0.3800\n",
      "Epoch 905/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0077 - val_loss: 0.4126\n",
      "Epoch 906/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.4116\n",
      "Epoch 907/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0106 - val_loss: 0.4219\n",
      "Epoch 908/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0120 - val_loss: 0.3911\n",
      "Epoch 909/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0115 - val_loss: 0.3947\n",
      "Epoch 910/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0139 - val_loss: 0.3836\n",
      "Epoch 911/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0114 - val_loss: 0.3773\n",
      "Epoch 912/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0119 - val_loss: 0.3983\n",
      "Epoch 913/1426\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0104 - val_loss: 0.4024\n",
      "Epoch 914/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0094 - val_loss: 0.3696\n",
      "Epoch 915/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0083 - val_loss: 0.4019\n",
      "Epoch 916/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0073 - val_loss: 0.3755\n",
      "Epoch 917/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0086 - val_loss: 0.3985\n",
      "Epoch 918/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0081 - val_loss: 0.3917\n",
      "Epoch 919/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0078 - val_loss: 0.3887\n",
      "Epoch 920/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0078 - val_loss: 0.4011\n",
      "Epoch 921/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0082 - val_loss: 0.3898\n",
      "Epoch 922/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0077 - val_loss: 0.3966\n",
      "Epoch 923/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0059 - val_loss: 0.4001\n",
      "Epoch 924/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0052 - val_loss: 0.3899\n",
      "Epoch 925/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0044 - val_loss: 0.3959\n",
      "Epoch 926/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.3820\n",
      "Epoch 927/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0040 - val_loss: 0.4008\n",
      "Epoch 928/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.3851\n",
      "Epoch 929/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0041 - val_loss: 0.3852\n",
      "Epoch 930/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0036 - val_loss: 0.3995\n",
      "Epoch 931/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.3983\n",
      "Epoch 932/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0043 - val_loss: 0.3859\n",
      "Epoch 933/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0045 - val_loss: 0.3865\n",
      "Epoch 934/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0042 - val_loss: 0.4025\n",
      "Epoch 935/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0045 - val_loss: 0.4056\n",
      "Epoch 936/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0046 - val_loss: 0.3852\n",
      "Epoch 937/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0051 - val_loss: 0.3893\n",
      "Epoch 938/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0051 - val_loss: 0.4006\n",
      "Epoch 939/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0059 - val_loss: 0.4046\n",
      "Epoch 940/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0061 - val_loss: 0.3870\n",
      "Epoch 941/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0063 - val_loss: 0.3945\n",
      "Epoch 942/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0057 - val_loss: 0.3992\n",
      "Epoch 943/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0068 - val_loss: 0.4086\n",
      "Epoch 944/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0099 - val_loss: 0.3984\n",
      "Epoch 945/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0104 - val_loss: 0.3744\n",
      "Epoch 946/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0114 - val_loss: 0.3957\n",
      "Epoch 947/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.3730\n",
      "Epoch 948/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0159 - val_loss: 0.4008\n",
      "Epoch 949/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0145 - val_loss: 0.3964\n",
      "Epoch 950/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0157 - val_loss: 0.4142\n",
      "Epoch 951/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0224 - val_loss: 0.3967\n",
      "Epoch 952/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0273 - val_loss: 0.4262\n",
      "Epoch 953/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0240 - val_loss: 0.4253\n",
      "Epoch 954/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0390 - val_loss: 0.4302\n",
      "Epoch 955/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0288 - val_loss: 0.3652\n",
      "Epoch 956/1426\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.042 - 0s 19ms/step - loss: 0.0418 - val_loss: 0.4268\n",
      "Epoch 957/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0312 - val_loss: 0.4058\n",
      "Epoch 958/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0220 - val_loss: 0.4078\n",
      "Epoch 959/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0222 - val_loss: 0.4082\n",
      "Epoch 960/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0180 - val_loss: 0.3857\n",
      "Epoch 961/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0134 - val_loss: 0.4097\n",
      "Epoch 962/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0123 - val_loss: 0.3615\n",
      "Epoch 963/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0089 - val_loss: 0.3809\n",
      "Epoch 964/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0068 - val_loss: 0.3741\n",
      "Epoch 965/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0054 - val_loss: 0.3716\n",
      "Epoch 966/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0042 - val_loss: 0.3816\n",
      "Epoch 967/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0037 - val_loss: 0.3697\n",
      "Epoch 968/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0033 - val_loss: 0.3725\n",
      "Epoch 969/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0029 - val_loss: 0.3713\n",
      "Epoch 970/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0029 - val_loss: 0.3720\n",
      "Epoch 971/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0026 - val_loss: 0.3697\n",
      "Epoch 972/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0026 - val_loss: 0.3698\n",
      "Epoch 973/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0025 - val_loss: 0.3723\n",
      "Epoch 974/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0023 - val_loss: 0.3723\n",
      "Epoch 975/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0023 - val_loss: 0.3704\n",
      "Epoch 976/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3745\n",
      "Epoch 977/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3749\n",
      "Epoch 978/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0021 - val_loss: 0.3722\n",
      "Epoch 979/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0020 - val_loss: 0.3743\n",
      "Epoch 980/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.3777\n",
      "Epoch 981/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.3739\n",
      "Epoch 982/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3746\n",
      "Epoch 983/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3771\n",
      "Epoch 984/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0018 - val_loss: 0.3790\n",
      "Epoch 985/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0019 - val_loss: 0.3744\n",
      "Epoch 986/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0018 - val_loss: 0.3748\n",
      "Epoch 987/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0019 - val_loss: 0.3798\n",
      "Epoch 988/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3830\n",
      "Epoch 989/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0021 - val_loss: 0.3762\n",
      "Epoch 990/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.3753\n",
      "Epoch 991/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0021 - val_loss: 0.3869\n",
      "Epoch 992/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0023 - val_loss: 0.3916\n",
      "Epoch 993/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0025 - val_loss: 0.3809\n",
      "Epoch 994/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0028 - val_loss: 0.3710\n",
      "Epoch 995/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.3850\n",
      "Epoch 996/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0028 - val_loss: 0.3961\n",
      "Epoch 997/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0033 - val_loss: 0.3783\n",
      "Epoch 998/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0033 - val_loss: 0.3723\n",
      "Epoch 999/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0042 - val_loss: 0.3880\n",
      "Epoch 1000/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0040 - val_loss: 0.3905\n",
      "Epoch 1001/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0045 - val_loss: 0.3813\n",
      "Epoch 1002/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0044 - val_loss: 0.3776\n",
      "Epoch 1003/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0049 - val_loss: 0.3860\n",
      "Epoch 1004/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0043 - val_loss: 0.3914\n",
      "Epoch 1005/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0054 - val_loss: 0.3938\n",
      "Epoch 1006/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0049 - val_loss: 0.3775\n",
      "Epoch 1007/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0049 - val_loss: 0.3835\n",
      "Epoch 1008/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0051 - val_loss: 0.3783\n",
      "Epoch 1009/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.3820\n",
      "Epoch 1010/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0046 - val_loss: 0.3737\n",
      "Epoch 1011/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0040 - val_loss: 0.3913\n",
      "Epoch 1012/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0054 - val_loss: 0.3903\n",
      "Epoch 1013/1426\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.004 - 0s 19ms/step - loss: 0.0045 - val_loss: 0.3679\n",
      "Epoch 1014/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0043 - val_loss: 0.3721\n",
      "Epoch 1015/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0035 - val_loss: 0.3656\n",
      "Epoch 1016/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.3929\n",
      "Epoch 1017/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0034 - val_loss: 0.3732\n",
      "Epoch 1018/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0031 - val_loss: 0.3715\n",
      "Epoch 1019/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0032 - val_loss: 0.3876\n",
      "Epoch 1020/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0032 - val_loss: 0.3931\n",
      "Epoch 1021/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0032 - val_loss: 0.3761\n",
      "Epoch 1022/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0033 - val_loss: 0.3725\n",
      "Epoch 1023/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.3936\n",
      "Epoch 1024/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0037 - val_loss: 0.3938\n",
      "Epoch 1025/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0041 - val_loss: 0.3715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1026/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0043 - val_loss: 0.3778\n",
      "Epoch 1027/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.3852\n",
      "Epoch 1028/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0036 - val_loss: 0.3746\n",
      "Epoch 1029/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0035 - val_loss: 0.3753\n",
      "Epoch 1030/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0043 - val_loss: 0.3933\n",
      "Epoch 1031/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.3809\n",
      "Epoch 1032/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0040 - val_loss: 0.3736\n",
      "Epoch 1033/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0038 - val_loss: 0.3912\n",
      "Epoch 1034/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0036 - val_loss: 0.3876\n",
      "Epoch 1035/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0035 - val_loss: 0.3773\n",
      "Epoch 1036/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0035 - val_loss: 0.3959\n",
      "Epoch 1037/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0050 - val_loss: 0.4018\n",
      "Epoch 1038/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0044 - val_loss: 0.3804\n",
      "Epoch 1039/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0077 - val_loss: 0.3834\n",
      "Epoch 1040/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0114 - val_loss: 0.4114\n",
      "Epoch 1041/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0099 - val_loss: 0.3883\n",
      "Epoch 1042/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0119 - val_loss: 0.3736\n",
      "Epoch 1043/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0156 - val_loss: 0.3803\n",
      "Epoch 1044/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0204 - val_loss: 0.3886\n",
      "Epoch 1045/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0300 - val_loss: 0.4014\n",
      "Epoch 1046/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0484 - val_loss: 0.3676\n",
      "Epoch 1047/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0600 - val_loss: 0.4275\n",
      "Epoch 1048/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0503 - val_loss: 0.4564\n",
      "Epoch 1049/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0332 - val_loss: 0.3992\n",
      "Epoch 1050/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0297 - val_loss: 0.4104\n",
      "Epoch 1051/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0413 - val_loss: 0.3977\n",
      "Epoch 1052/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0331 - val_loss: 0.3763\n",
      "Epoch 1053/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0378 - val_loss: 0.3685\n",
      "Epoch 1054/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0280 - val_loss: 0.3956\n",
      "Epoch 1055/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0233 - val_loss: 0.3769\n",
      "Epoch 1056/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0171 - val_loss: 0.3645\n",
      "Epoch 1057/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0131 - val_loss: 0.3735\n",
      "Epoch 1058/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0092 - val_loss: 0.3706\n",
      "Epoch 1059/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0075 - val_loss: 0.3744\n",
      "Epoch 1060/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0064 - val_loss: 0.3683\n",
      "Epoch 1061/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0057 - val_loss: 0.3698\n",
      "Epoch 1062/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0051 - val_loss: 0.3716\n",
      "Epoch 1063/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0046 - val_loss: 0.3740\n",
      "Epoch 1064/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0040 - val_loss: 0.3744\n",
      "Epoch 1065/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0035 - val_loss: 0.3793\n",
      "Epoch 1066/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0029 - val_loss: 0.3810\n",
      "Epoch 1067/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0024 - val_loss: 0.3831\n",
      "Epoch 1068/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3833\n",
      "Epoch 1069/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.3852\n",
      "Epoch 1070/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0019 - val_loss: 0.3852\n",
      "Epoch 1071/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0018 - val_loss: 0.3865\n",
      "Epoch 1072/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0017 - val_loss: 0.3861\n",
      "Epoch 1073/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0017 - val_loss: 0.3872\n",
      "Epoch 1074/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.3874\n",
      "Epoch 1075/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.3873\n",
      "Epoch 1076/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3879\n",
      "Epoch 1077/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3878\n",
      "Epoch 1078/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3887\n",
      "Epoch 1079/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3884\n",
      "Epoch 1080/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3892\n",
      "Epoch 1081/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3901\n",
      "Epoch 1082/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3893\n",
      "Epoch 1083/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3899\n",
      "Epoch 1084/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3904\n",
      "Epoch 1085/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3901\n",
      "Epoch 1086/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3900\n",
      "Epoch 1087/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3902\n",
      "Epoch 1088/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3904\n",
      "Epoch 1089/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3913\n",
      "Epoch 1090/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3889\n",
      "Epoch 1091/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3914\n",
      "Epoch 1092/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3896\n",
      "Epoch 1093/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0012 - val_loss: 0.3923\n",
      "Epoch 1094/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0013 - val_loss: 0.3903\n",
      "Epoch 1095/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3907\n",
      "Epoch 1096/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0013 - val_loss: 0.39243320e\n",
      "Epoch 1097/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3923\n",
      "Epoch 1098/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3890\n",
      "Epoch 1099/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3913\n",
      "Epoch 1100/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0013 - val_loss: 0.3918\n",
      "Epoch 1101/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3906\n",
      "Epoch 1102/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3900\n",
      "Epoch 1103/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0015 - val_loss: 0.3885\n",
      "Epoch 1104/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3933\n",
      "Epoch 1105/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3886\n",
      "Epoch 1106/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3911\n",
      "Epoch 1107/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3865\n",
      "Epoch 1108/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3935\n",
      "Epoch 1109/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3880\n",
      "Epoch 1110/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3929\n",
      "Epoch 1111/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3840\n",
      "Epoch 1112/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3934\n",
      "Epoch 1113/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3908\n",
      "Epoch 1114/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3882\n",
      "Epoch 1115/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0013 - val_loss: 0.3868\n",
      "Epoch 1116/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3930\n",
      "Epoch 1117/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3906\n",
      "Epoch 1118/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0016 - val_loss: 0.3921\n",
      "Epoch 1119/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3858\n",
      "Epoch 1120/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0018 - val_loss: 0.3951\n",
      "Epoch 1121/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3943\n",
      "Epoch 1122/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0026 - val_loss: 0.3898\n",
      "Epoch 1123/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0036 - val_loss: 0.3889\n",
      "Epoch 1124/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0052 - val_loss: 0.4016\n",
      "Epoch 1125/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0073 - val_loss: 0.4036\n",
      "Epoch 1126/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0119 - val_loss: 0.3857\n",
      "Epoch 1127/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0118 - val_loss: 0.3843\n",
      "Epoch 1128/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0108 - val_loss: 0.3796\n",
      "Epoch 1129/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0110 - val_loss: 0.4033\n",
      "Epoch 1130/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0107 - val_loss: 0.3817\n",
      "Epoch 1131/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0083 - val_loss: 0.3941\n",
      "Epoch 1132/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0068 - val_loss: 0.3752\n",
      "Epoch 1133/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0054 - val_loss: 0.3772\n",
      "Epoch 1134/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0049 - val_loss: 0.3960\n",
      "Epoch 1135/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.3733\n",
      "Epoch 1136/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0032 - val_loss: 0.3682\n",
      "Epoch 1137/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0033 - val_loss: 0.3913\n",
      "Epoch 1138/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0025 - val_loss: 0.3800\n",
      "Epoch 1139/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0023 - val_loss: 0.3894\n",
      "Epoch 1140/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0019 - val_loss: 0.3736\n",
      "Epoch 1141/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0017 - val_loss: 0.3826\n",
      "Epoch 1142/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3895\n",
      "Epoch 1143/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3887\n",
      "Epoch 1144/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3822\n",
      "Epoch 1145/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3822\n",
      "Epoch 1146/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0015 - val_loss: 0.3860\n",
      "Epoch 1147/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3885\n",
      "Epoch 1148/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3914\n",
      "Epoch 1149/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3827\n",
      "Epoch 1150/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3786\n",
      "Epoch 1151/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0024 - val_loss: 0.3917\n",
      "Epoch 1152/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.3956\n",
      "Epoch 1153/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.3871\n",
      "Epoch 1154/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0031 - val_loss: 0.3767\n",
      "Epoch 1155/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0031 - val_loss: 0.3837\n",
      "Epoch 1156/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0036 - val_loss: 0.3990\n",
      "Epoch 1157/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.3954\n",
      "Epoch 1158/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.3776\n",
      "Epoch 1159/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0045 - val_loss: 0.3895\n",
      "Epoch 1160/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0046 - val_loss: 0.4001\n",
      "Epoch 1161/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0056 - val_loss: 0.3955\n",
      "Epoch 1162/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0060 - val_loss: 0.3766\n",
      "Epoch 1163/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0084 - val_loss: 0.3908\n",
      "Epoch 1164/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0088 - val_loss: 0.4039\n",
      "Epoch 1165/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0109 - val_loss: 0.3975\n",
      "Epoch 1166/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0100 - val_loss: 0.3905\n",
      "Epoch 1167/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0099 - val_loss: 0.3872\n",
      "Epoch 1168/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0103 - val_loss: 0.3765\n",
      "Epoch 1169/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0089 - val_loss: 0.4080\n",
      "Epoch 1170/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0062 - val_loss: 0.3897\n",
      "Epoch 1171/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0035 - val_loss: 0.3883\n",
      "Epoch 1172/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0029 - val_loss: 0.3828\n",
      "Epoch 1173/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0023 - val_loss: 0.3889\n",
      "Epoch 1174/1426\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.001 - 0s 20ms/step - loss: 0.0018 - val_loss: 0.3832\n",
      "Epoch 1175/1426\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.0017 - val_loss: 0.3819\n",
      "Epoch 1176/1426\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.0015 - val_loss: 0.3866\n",
      "Epoch 1177/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0013 - val_loss: 0.3867\n",
      "Epoch 1178/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0012 - val_loss: 0.3827\n",
      "Epoch 1179/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3858\n",
      "Epoch 1180/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.3890\n",
      "Epoch 1181/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3823\n",
      "Epoch 1182/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0010 - val_loss: 0.3826\n",
      "Epoch 1183/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.3909\n",
      "Epoch 1184/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0011 - val_loss: 0.3850\n",
      "Epoch 1185/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0011 - val_loss: 0.3850\n",
      "Epoch 1186/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3904\n",
      "Epoch 1187/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3882\n",
      "Epoch 1188/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3861\n",
      "Epoch 1189/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3915\n",
      "Epoch 1190/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3858\n",
      "Epoch 1191/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3848\n",
      "Epoch 1192/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3909\n",
      "Epoch 1193/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0016 - val_loss: 0.3945\n",
      "Epoch 1194/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3849\n",
      "Epoch 1195/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.3847\n",
      "Epoch 1196/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0018 - val_loss: 0.3928\n",
      "Epoch 1197/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3917\n",
      "Epoch 1198/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3883\n",
      "Epoch 1199/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0021 - val_loss: 0.3912\n",
      "Epoch 1200/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.3919\n",
      "Epoch 1201/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0023 - val_loss: 0.3926\n",
      "Epoch 1202/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0024 - val_loss: 0.3923\n",
      "Epoch 1203/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0024 - val_loss: 0.3865\n",
      "Epoch 1204/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0028 - val_loss: 0.3849\n",
      "Epoch 1205/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0030 - val_loss: 0.3822\n",
      "Epoch 1206/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0038 - val_loss: 0.4068\n",
      "Epoch 1207/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0039 - val_loss: 0.3851\n",
      "Epoch 1208/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0041 - val_loss: 0.4021\n",
      "Epoch 1209/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0045 - val_loss: 0.3839\n",
      "Epoch 1210/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0048 - val_loss: 0.3956\n",
      "Epoch 1211/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0047 - val_loss: 0.3820\n",
      "Epoch 1212/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.3908\n",
      "Epoch 1213/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0037 - val_loss: 0.3861\n",
      "Epoch 1214/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0048 - val_loss: 0.3967\n",
      "Epoch 1215/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0053 - val_loss: 0.3784\n",
      "Epoch 1216/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0046 - val_loss: 0.3806\n",
      "Epoch 1217/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0056 - val_loss: 0.3851\n",
      "Epoch 1218/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0078 - val_loss: 0.4058\n",
      "Epoch 1219/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0082 - val_loss: 0.3997\n",
      "Epoch 1220/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0114 - val_loss: 0.3883\n",
      "Epoch 1221/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0152 - val_loss: 0.4082\n",
      "Epoch 1222/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0234 - val_loss: 0.3858\n",
      "Epoch 1223/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0251 - val_loss: 0.4124\n",
      "Epoch 1224/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0273 - val_loss: 0.3965\n",
      "Epoch 1225/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0252 - val_loss: 0.3754\n",
      "Epoch 1226/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0238 - val_loss: 0.3925\n",
      "Epoch 1227/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0185 - val_loss: 0.3732\n",
      "Epoch 1228/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0115 - val_loss: 0.3991\n",
      "Epoch 1229/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0159 - val_loss: 0.3823\n",
      "Epoch 1230/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0112 - val_loss: 0.4089\n",
      "Epoch 1231/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0100 - val_loss: 0.3942\n",
      "Epoch 1232/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0062 - val_loss: 0.3867\n",
      "Epoch 1233/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0042 - val_loss: 0.3823\n",
      "Epoch 1234/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0028 - val_loss: 0.3849\n",
      "Epoch 1235/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3736\n",
      "Epoch 1236/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3752\n",
      "Epoch 1237/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0017 - val_loss: 0.3759\n",
      "Epoch 1238/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3736\n",
      "Epoch 1239/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0014 - val_loss: 0.3746\n",
      "Epoch 1240/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0013 - val_loss: 0.3771\n",
      "Epoch 1241/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3781\n",
      "Epoch 1242/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.3762\n",
      "Epoch 1243/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0011 - val_loss: 0.3774\n",
      "Epoch 1244/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3825\n",
      "Epoch 1245/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0010 - val_loss: 0.3805\n",
      "Epoch 1246/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3778\n",
      "Epoch 1247/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 9.7025e-04 - val_loss: 0.3819\n",
      "Epoch 1248/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 9.4836e-04 - val_loss: 0.3839\n",
      "Epoch 1249/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.9588e-04 - val_loss: 0.3797\n",
      "Epoch 1250/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 9.1152e-04 - val_loss: 0.3808\n",
      "Epoch 1251/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 9.1017e-04 - val_loss: 0.3852\n",
      "Epoch 1252/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.5263e-04 - val_loss: 0.3845\n",
      "Epoch 1253/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.8245e-04 - val_loss: 0.3805\n",
      "Epoch 1254/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.9044e-04 - val_loss: 0.3837\n",
      "Epoch 1255/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 9.0579e-04 - val_loss: 0.3872\n",
      "Epoch 1256/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 8.5669e-04 - val_loss: 0.3842\n",
      "Epoch 1257/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 9.3979e-04 - val_loss: 0.3806\n",
      "Epoch 1258/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 9.4291e-04 - val_loss: 0.3869\n",
      "Epoch 1259/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 9.8516e-04 - val_loss: 0.3898\n",
      "Epoch 1260/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 19ms/step - loss: 9.4839e-04 - val_loss: 0.3823\n",
      "Epoch 1261/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3828\n",
      "Epoch 1262/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.3904\n",
      "Epoch 1263/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3893\n",
      "Epoch 1264/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0012 - val_loss: 0.3813\n",
      "Epoch 1265/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3837\n",
      "Epoch 1266/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3937\n",
      "Epoch 1267/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0015 - val_loss: 0.3875\n",
      "Epoch 1268/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0016 - val_loss: 0.3809\n",
      "Epoch 1269/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0018 - val_loss: 0.3860\n",
      "Epoch 1270/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0020 - val_loss: 0.3951\n",
      "Epoch 1271/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0018 - val_loss: 0.3857\n",
      "Epoch 1272/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.3819\n",
      "Epoch 1273/1426\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.0022 - val_loss: 0.3912\n",
      "Epoch 1274/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0027 - val_loss: 0.3978\n",
      "Epoch 1275/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0023 - val_loss: 0.3791\n",
      "Epoch 1276/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0026 - val_loss: 0.380502\n",
      "Epoch 1277/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0030 - val_loss: 0.3988\n",
      "Epoch 1278/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0030 - val_loss: 0.3977\n",
      "Epoch 1279/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.3825\n",
      "Epoch 1280/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0026 - val_loss: 0.3866\n",
      "Epoch 1281/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0032 - val_loss: 0.4011\n",
      "Epoch 1282/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0026 - val_loss: 0.3792\n",
      "Epoch 1283/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.3875\n",
      "Epoch 1284/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0030 - val_loss: 0.3949\n",
      "Epoch 1285/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0030 - val_loss: 0.3992\n",
      "Epoch 1286/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0031 - val_loss: 0.3714\n",
      "Epoch 1287/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.3832\n",
      "Epoch 1288/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0029 - val_loss: 0.3945\n",
      "Epoch 1289/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0026 - val_loss: 0.3722\n",
      "Epoch 1290/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0024 - val_loss: 0.3853\n",
      "Epoch 1291/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0024 - val_loss: 0.3885\n",
      "Epoch 1292/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0022 - val_loss: 0.3841\n",
      "Epoch 1293/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0022 - val_loss: 0.3801\n",
      "Epoch 1294/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0020 - val_loss: 0.3832\n",
      "Epoch 1295/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0017 - val_loss: 0.3880\n",
      "Epoch 1296/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3722\n",
      "Epoch 1297/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0016 - val_loss: 0.3819\n",
      "Epoch 1298/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3833\n",
      "Epoch 1299/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3733\n",
      "Epoch 1300/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3843\n",
      "Epoch 1301/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3824\n",
      "Epoch 1302/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0017 - val_loss: 0.3782\n",
      "Epoch 1303/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0021 - val_loss: 0.3725\n",
      "Epoch 1304/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3888\n",
      "Epoch 1305/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0025 - val_loss: 0.3706\n",
      "Epoch 1306/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0023 - val_loss: 0.3892\n",
      "Epoch 1307/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0031 - val_loss: 0.3713\n",
      "Epoch 1308/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0034 - val_loss: 0.3863\n",
      "Epoch 1309/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0041 - val_loss: 0.3777\n",
      "Epoch 1310/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0036 - val_loss: 0.3808\n",
      "Epoch 1311/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0045 - val_loss: 0.3666\n",
      "Epoch 1312/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0044 - val_loss: 0.3879\n",
      "Epoch 1313/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0053 - val_loss: 0.3763\n",
      "Epoch 1314/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0058 - val_loss: 0.3875\n",
      "Epoch 1315/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.0070 - val_loss: 0.3597\n",
      "Epoch 1316/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0077 - val_loss: 0.3980\n",
      "Epoch 1317/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0076 - val_loss: 0.3690\n",
      "Epoch 1318/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0050 - val_loss: 0.3726\n",
      "Epoch 1319/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0043 - val_loss: 0.3793\n",
      "Epoch 1320/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0040 - val_loss: 0.3767\n",
      "Epoch 1321/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0036 - val_loss: 0.3694\n",
      "Epoch 1322/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0032 - val_loss: 0.3654\n",
      "Epoch 1323/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0029 - val_loss: 0.3668\n",
      "Epoch 1324/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0029 - val_loss: 0.3634\n",
      "Epoch 1325/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0025 - val_loss: 0.3673\n",
      "Epoch 1326/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0021 - val_loss: 0.3683\n",
      "Epoch 1327/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0019 - val_loss: 0.3670\n",
      "Epoch 1328/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0017 - val_loss: 0.3686\n",
      "Epoch 1329/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3661\n",
      "Epoch 1330/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3645\n",
      "Epoch 1331/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3648\n",
      "Epoch 1332/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0014 - val_loss: 0.3702\n",
      "Epoch 1333/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0013 - val_loss: 0.3677\n",
      "Epoch 1334/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.3678\n",
      "Epoch 1335/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3709\n",
      "Epoch 1336/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0013 - val_loss: 0.3687\n",
      "Epoch 1337/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3676\n",
      "Epoch 1338/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.3679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1339/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.3734\n",
      "Epoch 1340/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.3645\n",
      "Epoch 1341/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0010 - val_loss: 0.3744\n",
      "Epoch 1342/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0011 - val_loss: 0.3667\n",
      "Epoch 1343/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0011 - val_loss: 0.3689\n",
      "Epoch 1344/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3724\n",
      "Epoch 1345/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3728\n",
      "Epoch 1346/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0013 - val_loss: 0.3659\n",
      "Epoch 1347/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0012 - val_loss: 0.3701\n",
      "Epoch 1348/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3721\n",
      "Epoch 1349/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3692e\n",
      "Epoch 1350/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.3670\n",
      "Epoch 1351/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0016 - val_loss: 0.3749\n",
      "Epoch 1352/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0019 - val_loss: 0.3676\n",
      "Epoch 1353/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.3679\n",
      "Epoch 1354/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.3692\n",
      "Epoch 1355/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0013 - val_loss: 0.3698\n",
      "Epoch 1356/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0018 - val_loss: 0.3758\n",
      "Epoch 1357/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0018 - val_loss: 0.3718\n",
      "Epoch 1358/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0022 - val_loss: 0.3692\n",
      "Epoch 1359/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0022 - val_loss: 0.3708\n",
      "Epoch 1360/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3733\n",
      "Epoch 1361/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.3742\n",
      "Epoch 1362/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0019 - val_loss: 0.3701\n",
      "Epoch 1363/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0018 - val_loss: 0.3695\n",
      "Epoch 1364/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.3799\n",
      "Epoch 1365/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0021 - val_loss: 0.3675\n",
      "Epoch 1366/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0021 - val_loss: 0.3697\n",
      "Epoch 1367/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0019 - val_loss: 0.3745\n",
      "Epoch 1368/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3677\n",
      "Epoch 1369/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0021 - val_loss: 0.3701\n",
      "Epoch 1370/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0022 - val_loss: 0.3634\n",
      "Epoch 1371/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0023 - val_loss: 0.3618\n",
      "Epoch 1372/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0023 - val_loss: 0.3760\n",
      "Epoch 1373/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0023 - val_loss: 0.3605\n",
      "Epoch 1374/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0023 - val_loss: 0.3929\n",
      "Epoch 1375/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.3749\n",
      "Epoch 1376/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0026 - val_loss: 0.3808\n",
      "Epoch 1377/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0041 - val_loss: 0.3629\n",
      "Epoch 1378/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0041 - val_loss: 0.4000\n",
      "Epoch 1379/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0057 - val_loss: 0.3683\n",
      "Epoch 1380/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0056 - val_loss: 0.3766\n",
      "Epoch 1381/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0066 - val_loss: 0.3727\n",
      "Epoch 1382/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0101 - val_loss: 0.4042\n",
      "Epoch 1383/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0192 - val_loss: 0.3859\n",
      "Epoch 1384/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0429 - val_loss: 0.3802\n",
      "Epoch 1385/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0487 - val_loss: 0.4023\n",
      "Epoch 1386/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0805 - val_loss: 0.3590\n",
      "Epoch 1387/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0769 - val_loss: 0.4033\n",
      "Epoch 1388/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0593 - val_loss: 0.3584\n",
      "Epoch 1389/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0442 - val_loss: 0.3547\n",
      "Epoch 1390/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0329 - val_loss: 0.3740\n",
      "Epoch 1391/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0230 - val_loss: 0.3998\n",
      "Epoch 1392/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.0211 - val_loss: 0.4320\n",
      "Epoch 1393/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0163 - val_loss: 0.3772\n",
      "Epoch 1394/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.4104\n",
      "Epoch 1395/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0070 - val_loss: 0.3967\n",
      "Epoch 1396/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0050 - val_loss: 0.3908\n",
      "Epoch 1397/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.3843\n",
      "Epoch 1398/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0029 - val_loss: 0.3833\n",
      "Epoch 1399/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0025 - val_loss: 0.3839\n",
      "Epoch 1400/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0022 - val_loss: 0.3831\n",
      "Epoch 1401/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.3850\n",
      "Epoch 1402/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0018 - val_loss: 0.3856\n",
      "Epoch 1403/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.3862\n",
      "Epoch 1404/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0015 - val_loss: 0.3872\n",
      "Epoch 1405/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0014 - val_loss: 0.3872\n",
      "Epoch 1406/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0013 - val_loss: 0.3874\n",
      "Epoch 1407/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.3872\n",
      "Epoch 1408/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0012 - val_loss: 0.3867\n",
      "Epoch 1409/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3865\n",
      "Epoch 1410/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0011 - val_loss: 0.3861\n",
      "Epoch 1411/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0010 - val_loss: 0.3854\n",
      "Epoch 1412/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 9.6924e-04 - val_loss: 0.3849\n",
      "Epoch 1413/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 9.2757e-04 - val_loss: 0.3845\n",
      "Epoch 1414/1426\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 8.8877e-04 - val_loss: 0.3838\n",
      "Epoch 1415/1426\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 8.5247e-04 - val_loss: 0.3835\n",
      "Epoch 1416/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 8.1797e-04 - val_loss: 0.3832\n",
      "Epoch 1417/1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 20ms/step - loss: 7.8677e-04 - val_loss: 0.3826\n",
      "Epoch 1418/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 7.5703e-04 - val_loss: 0.3824\n",
      "Epoch 1419/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 7.3002e-04 - val_loss: 0.3823\n",
      "Epoch 1420/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 7.0457e-04 - val_loss: 0.3820\n",
      "Epoch 1421/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 6.8087e-04 - val_loss: 0.3820\n",
      "Epoch 1422/1426\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 6.5881e-04 - val_loss: 0.3821\n",
      "Epoch 1423/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 6.3805e-04 - val_loss: 0.3819\n",
      "Epoch 1424/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 6.1872e-04 - val_loss: 0.3819\n",
      "Epoch 1425/1426\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 6.0032e-04 - val_loss: 0.3819\n",
      "Epoch 1426/1426\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 5.8345e-04 - val_loss: 0.3817\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (89, 128)                 66560     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (89, 1)                   129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,689\n",
      "Trainable params: 66,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_anom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-130eb2d03c18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_cs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bucket'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtest_anom\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'actual'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;31m# fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= tuner_pred[0], name='tuner_prediction'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_cs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bucket'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'model_prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_anom' is not defined"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "# conn =sqlite3.connect('sqlite_8.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "# print(c.fetchall())\n",
    "\n",
    "# c.execute(\"SELECT * FROM syslogs ORDER BY timestamp DESC LIMIT 1\")\n",
    "# print(c.fetchall())\n",
    "\n",
    "\n",
    "limit = '16'\n",
    "bucket_size = 10\n",
    "timing =90\n",
    "\n",
    "\n",
    "variable = int(60*bucket_size)\n",
    "\n",
    "df = pd.read_pickle(\"dataframe.gz\")\n",
    "\n",
    "\n",
    "def makeDataframe(df):\n",
    "    import datetime\n",
    "\n",
    "    atype = str(df.tail(1)['bucket'].values[0])\n",
    "    ctype = str(df.head(1)['bucket'].values[0])\n",
    "\n",
    "    sample_series = []\n",
    "\n",
    "    end = datetime.datetime.strptime(atype, '%Y-%m-%d %H:%M:%S')\n",
    "    # start = end - datetime.timedelta(minutes = 3000)\n",
    "    start = datetime.datetime.strptime(ctype, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    delta = datetime.timedelta(minutes=bucket_size)\n",
    "    # start = datetime.datetime.strptime( start, '%Y-%m-%d %H:%M:%S' )\n",
    "    # end = datetime.datetime.strptime( end, '%Y-%m-%d %H:%M:%S' )\n",
    "    t = start\n",
    "    while t <= end :\n",
    "        sample_series.append(datetime.datetime.strftime( t, '%Y-%m-%d %H:%M:%S'))\n",
    "        t += delta\n",
    "    sample = pd.DataFrame(sample_series)\n",
    "    sample =sample.rename(columns={0:\"bucket\"})\n",
    "\n",
    "    sample.insert(1, 'msg_freq',0)\n",
    "\n",
    "    merged_df = df.merge(sample, how = 'outer', on = ['bucket'])\n",
    "    new_df = merged_df.sort_values('bucket')\n",
    "    new_df = new_df.drop(columns = ['msg_freq_y']).fillna(0)\n",
    "\n",
    "    new_df  = new_df.reset_index()\n",
    "    new_df = new_df.iloc[:,1:]\n",
    "    df = new_df.rename(columns={\"msg_freq_x\": \"msg_freq\"})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cumulativeFreq(df):\n",
    "    df_cs = df.copy()\n",
    "    df_cs['bucket'] = pd.to_datetime(df_cs['bucket'])\n",
    "    df_cs[\"msg_freq\"] = df_cs[\"msg_freq\"].iloc[:].groupby([df_cs['bucket'].dt.hour, df_cs['bucket'].dt.date]).cumsum()\n",
    "    \n",
    "    return df_cs\n",
    "\n",
    "\n",
    "\n",
    "def Normalizer(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    df1 = df.iloc[:,1:]   #change required\n",
    "#     values = dff.values\n",
    "#     values = values.astype('float32')\n",
    "#     # normalize features\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaled = scaler.fit_transform(values)\n",
    "#     dff = pd.DataFrame(scaled, columns=df.columns[1:]) #change required\n",
    "    dff = df1/df1.max()\n",
    "    dff.insert(0, 'bucket', df[\"bucket\"]) #change required\n",
    "    \n",
    "    return dff\n",
    "\n",
    "\n",
    "def sigmoidDataframe(df):\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        a=0.7\n",
    "        return 1/(1+5*np.exp(-a*x))\n",
    "    \n",
    "    dff = df.copy()\n",
    "    dff['msg_freq'] = sigmoid(dff['msg_freq'])\n",
    "    return dff\n",
    "\n",
    "df = makeDataframe(df)\n",
    "df_cs = cumulativeFreq(df)\n",
    "\n",
    "dfn = Normalizer(df)\n",
    "dfn_cs = Normalizer(df_cs)\n",
    "\n",
    "\n",
    "Values = df_cs.iloc[:,1].values\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "# choose a number of time steps; basically the memory of the network\n",
    "n_steps = int(timing/bucket_size)\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(Values, n_steps)\n",
    "\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "\n",
    "def wisesplit(num,div):\n",
    "    \n",
    "    part = num - div\n",
    "    while part/num > 0.8:\n",
    "        part = part-div\n",
    "    \n",
    "    return part\n",
    "\n",
    "def print_factors(num):\n",
    "    import numpy as np\n",
    "    factors = []\n",
    "    for i in range(1, num + 1):\n",
    "        if num % i == 0:\n",
    "            factors.append(i)\n",
    "    return np.array(factors)\n",
    "\n",
    "arr =print_factors(len(X))\n",
    "div = arr[int(0.6*arr.shape[0])]\n",
    "\n",
    "split = wisesplit(len(X),div)\n",
    "# split =int(0.8*len(X))\n",
    "# split = -272\n",
    "\n",
    "train_X = np.array(X)[:split]\n",
    "train_y = np.array(y)[:split]\n",
    "test_X = np.array(X)[split:]\n",
    "test_y = np.array(y)[split:]\n",
    "\n",
    "train_y =train_y.reshape((train_y.shape[0],1))\n",
    "\n",
    "test_y = test_y.reshape((test_y.shape[0],1))\n",
    "\n",
    "def compute_hcf(x, y):\n",
    "\n",
    "# choose the smaller number\n",
    "    if x > y:\n",
    "        smaller = y\n",
    "    else:\n",
    "        smaller = x\n",
    "    for i in range(1, smaller+1):\n",
    "        if((x % i == 0) and (y % i == 0)):\n",
    "            hcf = i \n",
    "    return hcf\n",
    "\n",
    "batchsize = compute_hcf(train_X.shape[0],test_X.shape[0])\n",
    "batch_arr = print_factors(batchsize)\n",
    "bt = batch_arr[-1]\n",
    "\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "    \n",
    "    \n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_unit',min_value=32,max_value=128, step =32),return_sequences=False, batch_input_shape=(bt,train_X.shape[1],train_X.shape[2]), stateful=True))\n",
    "#     for i in range(hp.Int('n_layers', 1, 4)):\n",
    "#         model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=32,max_value=96,step=32),return_sequences=True))\n",
    "#     model.add(LSTM(hp.Int('layer_2_neurons',min_value=32,max_value=128,step=32)))\n",
    "#     model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    hp_loss = hp.Choice('loss', values=['mean_squared_logarithmic_error'])\n",
    "    model.add(Dense(train_y.shape[1]))\n",
    "    model.compile(loss=hp_loss, optimizer=keras.optimizers.Adam(hp.Float('learning_rate', min_value=0.0001,max_value=0.001, step =0.0001)))\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='loss',\n",
    "                     max_epochs=1500,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "print(tuner.search(\n",
    "        x=train_X,\n",
    "        y=train_y,\n",
    "        epochs=1500,\n",
    "        batch_size = bt,\n",
    "        validation_data=(test_X,test_y)\n",
    "))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "print(\"The optimal number of units in the first densely-connected\", best_hps.get('input_unit'))\n",
    "print(\"The optimal learning rate for the optimizer is \",best_hps.get('learning_rate') )\n",
    "print(\"The best loss is \", best_hps.get('loss'))\n",
    "\n",
    "\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(x=train_X, y=train_y, epochs=1500, validation_data=(test_X,test_y), batch_size=batchsize, verbose =0, shuffle=False)\n",
    "\n",
    "loss_per_epoch = history.history['loss']\n",
    "best_epoch = loss_per_epoch.index(min(loss_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "final_history = hypermodel.fit(x=train_X, y=train_y, epochs=best_epoch, validation_data=(test_X,test_y), batch_size=batchsize, verbose =1, shuffle=False)\n",
    "hyper_pred= hypermodel.predict(test_X,bt)\n",
    "\n",
    "#PLOTTING\n",
    "\n",
    "plt.figure(figsize=(12,8))   \n",
    "plt.plot(final_history.history['loss'],label='Training loss')\n",
    "plt.plot(final_history.history['val_loss'], label =\"Validation loss\")\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=10)\n",
    "#             plt.ylim(0,0.2)\n",
    "plt.legend(['Training loss', 'Validation loss'])\n",
    "plt.savefig('bestmodel_loss_10_90_layer1.pdf', dpi=250)\n",
    "plt.show()\n",
    "\n",
    "print(hypermodel.summary())\n",
    "hyper_pred_df = pd.DataFrame(hyper_pred.flatten())\n",
    "test_anom = pd.DataFrame(test_y)\n",
    "\n",
    "fig = go.Figure()    \n",
    "fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= test_anom[0], name='actual'))\n",
    "# fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= tuner_pred[0], name='tuner_prediction'))\n",
    "# fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= pred_df[0], name= 'model_prediction'))\n",
    "fig.add_trace(go.Scatter(x=df_cs['bucket'][split+n_steps:], y= hyper_pred_df[0], name='hypermodel_prediction'))\n",
    "fig.update_layout(showlegend=True, title='Frequency v/s Time',width=1600, height=800)\n",
    "fig.update_yaxes(title_text='Normalized frequency')\n",
    "fig.update_xaxes(title_text='Time')\n",
    "fig.update_layout(xaxis_range=['2022-01-09 12:00:00','2022-01-10 00:00:00'])\n",
    "# fig.update_layout(yaxis_range=[-0.1,1])\n",
    "fig.show()\n",
    "fig.write_image(\"bestmodel_predictions_10_90_layer1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
